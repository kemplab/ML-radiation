{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import keras\n",
    "#from keras import losses, regularizers\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout\n",
    "#from keras import backend as K\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['LogisticReg','SVM','GBM','NeuralNet']\n",
    "classifiers = ['GBM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hyperopt_iterations = 2**8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_trainvalidation_test = 20\n",
    "test_size = 0.2\n",
    "k_train_validation = 5\n",
    "early_stopping_size = 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1\n",
    "\n",
    "# implement seed\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_function(parameters):\n",
    "\n",
    "    # load data\n",
    "    with open('_files/data.pickle', 'rb') as f:\n",
    "        train_data, X_validation, y_validation = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # calculate performance\n",
    "    mean_validation_weightedlogloss = hyperopt_performance(train_data, X_validation, y_validation, parameters)\n",
    "    gc.collect()\n",
    "    \n",
    "    # save validation predictions if best classifier\n",
    "    with open('_files/validation.pickle','rb') as f:\n",
    "        best_weightedlogloss = pickle.load(f)\n",
    "    if mean_validation_weightedlogloss < best_weightedlogloss:\n",
    "        with open('_files/validation.pickle','wb') as f:\n",
    "            pickle.dump(mean_validation_weightedlogloss, f)\n",
    "    \n",
    "    # return performance\n",
    "    return {'loss':mean_validation_weightedlogloss, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_performance(train_data, X_validation, y_validation, parameters):\n",
    "    \n",
    "    # unpack train data\n",
    "    if classifiers[a] in ['GBM','NeuralNet']:\n",
    "        X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train = train_data[0], train_data[1], train_data[2], train_data[3]\n",
    "    else:\n",
    "        X_train, y_train = train_data[0], train_data[1]\n",
    "    \n",
    "    # initialize validation performance and predictions\n",
    "    validation_weightedlogloss = []\n",
    "    \n",
    "    # iterate over number of training/validation splits\n",
    "    for i in range(k_train_validation):\n",
    "        \n",
    "        # logistic regression\n",
    "        if classifiers[a] == 'LogisticReg':\n",
    "            \n",
    "            # create classifier\n",
    "            clf = LogisticRegression(penalty='elasticnet', class_weight='balanced', solver='saga', max_iter=10000, random_state=seed_, **parameters)\n",
    "            \n",
    "            # train on training\n",
    "            clf.fit(X_train[i], y_train[i])\n",
    "            \n",
    "            # evaluate on validation\n",
    "            y_pred = clf.predict_proba(X_validation[i])[:,1]\n",
    "            pos_weight = len([x for x in y_validation[i] if x==0])/len([x for x in y_validation[i] if x==1])\n",
    "            sample_weights = [pos_weight if x==1 else 1 for x in y_validation[i]]\n",
    "            weightedlogloss = log_loss(y_validation[i], y_pred, sample_weight=sample_weights)\n",
    "            validation_weightedlogloss.append(weightedlogloss)\n",
    "            \n",
    "        # SVM\n",
    "        elif classifiers[a] == 'SVM':\n",
    "            \n",
    "            # create classifier\n",
    "            clf = SVC(gamma='auto', probability=True, random_state=seed_, **parameters)\n",
    "            \n",
    "            # train on training\n",
    "            clf.fit(X_train[i], y_train[i])\n",
    "            \n",
    "            # evaluate on validation\n",
    "            y_pred = clf.predict_proba(X_validation[i])[:,1]\n",
    "            pos_weight = len([x for x in y_validation[i] if x==0])/len([x for x in y_validation[i] if x==1])\n",
    "            sample_weights = [pos_weight if x==1 else 1 for x in y_validation[i]]\n",
    "            weightedlogloss = log_loss(y_validation[i], y_pred, sample_weight=sample_weights)\n",
    "            validation_weightedlogloss.append(weightedlogloss)\n",
    "            \n",
    "        # GBM\n",
    "        elif classifiers[a] == 'GBM':\n",
    "\n",
    "            # positive weight\n",
    "            pos_weight = len([x for x in y_training_train[i] if x==0])/len([x for x in y_training_train[i] if x==1])\n",
    "\n",
    "            # xgb datasets\n",
    "            xgb_training = xgb.DMatrix(X_training_train[i], label=y_training_train[i])\n",
    "            xgb_earlystopping = xgb.DMatrix(X_earlystopping_train[i], label=y_earlystopping_train[i])\n",
    "            xgb_validation = xgb.DMatrix(X_validation[i], label=y_validation[i])\n",
    "\n",
    "            # parameters\n",
    "            param = parameters.copy()\n",
    "            param['objective'] = 'binary:logistic'\n",
    "            param['eval_metric'] = 'logloss'\n",
    "            param['scale_pos_weight'] = pos_weight\n",
    "            param['seed'] = seed_\n",
    "            evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "            # train on training\n",
    "            bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "            # evaluate on validation\n",
    "            y_pred = bst.predict(xgb_validation, ntree_limit=bst.best_ntree_limit)\n",
    "            pos_weight = len([x for x in y_validation[i] if x==0])/len([x for x in y_validation[i] if x==1])\n",
    "            sample_weights = [pos_weight if x==1 else 1 for x in y_validation[i]]\n",
    "            weightedlogloss = log_loss(y_validation[i], y_pred, sample_weight=sample_weights)\n",
    "            validation_weightedlogloss.append(weightedlogloss)\n",
    "            \n",
    "        # neural network\n",
    "        elif classifiers[a] == 'NeuralNet':\n",
    "            \n",
    "            # train best model on training+validation set\n",
    "            with tf.Graph().as_default():\n",
    "                with tf.Session() as sess:\n",
    "\n",
    "                    # create model\n",
    "                    model = Sequential()\n",
    "                    for j in range(parameters['number_of_layers']):\n",
    "                        model.add(Dense(parameters['neurons_per_layer'], activation=parameters['activation_function'], kernel_regularizer=regularizers.l1_l2(l1=parameters['l1'], l2=parameters['l2'])))\n",
    "                        model.add(Dropout(parameters['dropout_rate']))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                    # loss and performance metrics\n",
    "                    pos_weight = len([x for x in y_training_train[i] if x==0])/len([x for x in y_training_train[i] if x==1])\n",
    "                    model.compile(loss=weighted_cross_entropy(pos_weight), metrics=[weighted_cross_entropy(pos_weight)], optimizer=parameters['optimizer'])      \n",
    "                    earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "                    # fit and evaluate model\n",
    "                    model.fit(X_training_train[i], y_training_train[i], epochs=1000, verbose=0, validation_data=(X_earlystopping_train[i], y_earlystopping_train[i]), callbacks=[earlystopping])\n",
    "                    y_pred = model.predict_proba(X_validation[i], batch_size=len(y_validation[i]), verbose=0)\n",
    "                    pos_weight = len([x for x in y_validation[i] if x==0])/len([x for x in y_validation[i] if x==1])\n",
    "                    sample_weights = [pos_weight if x==1 else 1 for x in y_validation[i]]\n",
    "                    weightedlogloss = log_loss(y_validation[i], y_pred, sample_weight=sample_weights)\n",
    "                    validation_weightedlogloss.append(weightedlogloss)\n",
    "\n",
    "            # clear session\n",
    "            K.clear_session()\n",
    "\n",
    "    # average validation performance over all folds\n",
    "    mean_validation_weightedlogloss = np.mean(validation_weightedlogloss) + np.std(validation_weightedlogloss)/np.sqrt(len(validation_weightedlogloss))\n",
    "    return mean_validation_weightedlogloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_y(y):\n",
    "    \n",
    "    dummy_y_ = [[],[]]\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            dummy_y_[0].append(1)\n",
    "            dummy_y_[1].append(0)\n",
    "        else:\n",
    "            dummy_y_[0].append(0)\n",
    "            dummy_y_[1].append(1)\n",
    "    dummy_y_ = np.array(dummy_y_).T\n",
    "    return dummy_y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy(pos_weight):\n",
    "\n",
    "    # calculation of loss\n",
    "    def calculate_loss(y_true, y_pred):\n",
    "\n",
    "        # define loss\n",
    "        def define_loss(target, output):\n",
    "\n",
    "            import tensorflow as tf\n",
    "            from tensorflow.python.keras import backend as K\n",
    "            from tensorflow.python.ops import clip_ops, math_ops\n",
    "            from tensorflow.python.framework import ops\n",
    "\n",
    "            epsilon_ = ops.convert_to_tensor(K.epsilon(), dtype=output.dtype.base_dtype)\n",
    "            output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
    "            output = math_ops.log(output / (1 - output))\n",
    "            return tf.nn.weighted_cross_entropy_with_logits(targets=target, logits=output, pos_weight=pos_weight)\n",
    "\n",
    "        from tensorflow.python.keras import backend as K\n",
    "        return K.mean(define_loss(y_true,y_pred), axis=-1)\n",
    "\n",
    "    return calculate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gene lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene lists\n",
    "with open('../gene_lists/gene_lists.pickle','rb') as f:\n",
    "    genelists, genes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Lewis\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [05:25<00:00,  1.87s/it, best loss: 0.4983579227794873]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [07:25<00:00,  1.53s/it, best loss: 0.5223055591095852]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [08:34<00:00,  2.20s/it, best loss: 0.5268134566890962]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:35<00:00,  2.64s/it, best loss: 0.5282855703813735]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:40<00:00,  3.77s/it, best loss: 0.5321194635639418]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [06:45<00:00,  1.71s/it, best loss: 0.5146598834301176]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [06:59<00:00,  1.01it/s, best loss: 0.5054888257204557]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [07:28<00:00,  1.65s/it, best loss: 0.5372973448351177]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [08:46<00:00,  1.72s/it, best loss: 0.5074758254574623]\n",
      "Split 10\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [07:06<00:00,  1.84s/it, best loss: 0.501934294769931]\n",
      "Split 11\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [06:19<00:00,  2.94s/it, best loss: 0.518563959655159]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [06:48<00:00,  1.40s/it, best loss: 0.5252991125765478]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [08:14<00:00,  1.05s/it, best loss: 0.5414367874791114]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:11<00:00,  2.94s/it, best loss: 0.5188647951641145]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:16<00:00,  3.48s/it, best loss: 0.5070914626797285]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [08:09<00:00,  2.21s/it, best loss: 0.5303534677138011]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [06:39<00:00,  1.11s/it, best loss: 0.5213918659269384]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [06:18<00:00,  1.26it/s, best loss: 0.5108628867002509]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:39<00:00,  1.35s/it, best loss: 0.5221751412571934]\n",
      "Split 20\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [09:38<00:00,  2.76s/it, best loss: 0.532602495153192]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Torres-Roca 2005\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:13<00:00,  3.58it/s, best loss: 0.5543628477935106]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:33<00:00,  1.59it/s, best loss: 0.5714215212258025]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:09<00:00,  4.16it/s, best loss: 0.5658338331472849]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:52<00:00,  2.48it/s, best loss: 0.5690438992518101]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:03<00:00,  4.43it/s, best loss: 0.5774093182808981]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:40<00:00,  3.39it/s, best loss: 0.5655410971263566]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:45<00:00,  3.53it/s, best loss: 0.5680375383973985]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:18<00:00,  4.63it/s, best loss: 0.5878826058329025]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:10<00:00,  4.52it/s, best loss: 0.5653766266294032]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:15<00:00,  3.88it/s, best loss: 0.5509155862362652]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:14<00:00,  6.32it/s, best loss: 0.5597618177566286]\n",
      "Split 12\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:14<00:00,  5.96it/s, best loss: 0.584971773731706]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:03<00:00,  3.07it/s, best loss: 0.5899470615826389]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:25<00:00,  2.40it/s, best loss: 0.5798234823812779]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:38<00:00,  1.97it/s, best loss: 0.5700072851904352]\n",
      "Split 16\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:20<00:00,  4.07it/s, best loss: 0.59092745629583]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:14<00:00,  3.49it/s, best loss: 0.5681681952238122]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:04<00:00,  4.09it/s, best loss: 0.5809787550624562]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:05<00:00,  3.56it/s, best loss: 0.5690179674608132]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:09<00:00,  4.51it/s, best loss: 0.5779322304663063]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Nuyten 2006\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:41<00:00,  2.55it/s, best loss: 0.5336286338128418]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:54<00:00,  1.35s/it, best loss: 0.5444377228045988]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:48<00:00,  1.03it/s, best loss: 0.5465892008424601]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:07<00:00,  1.20it/s, best loss: 0.5501018953535035]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:38<00:00,  1.33s/it, best loss: 0.5596158761017265]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:48<00:00,  1.63it/s, best loss: 0.5536447910351723]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:45<00:00,  1.64it/s, best loss: 0.5532299573520794]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:33<00:00,  1.00s/it, best loss: 0.5760353800791816]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:27<00:00,  1.01s/it, best loss: 0.5568459060858897]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:04<00:00,  2.44it/s, best loss: 0.5454706031805677]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:06<00:00,  1.25it/s, best loss: 0.5473170553307543]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:24<00:00,  1.02s/it, best loss: 0.5675940722783119]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:34<00:00,  1.73s/it, best loss: 0.5601999098561454]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:04<00:00,  1.40it/s, best loss: 0.5671096174305474]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:03<00:00,  1.57it/s, best loss: 0.5393107798595251]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:30<00:00,  1.13it/s, best loss: 0.5614707980844323]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:41<00:00,  1.29it/s, best loss: 0.5485622139205859]\n",
      "Split 18\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:22<00:00,  2.21it/s, best loss: 0.530312831045199]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:21<00:00,  2.06it/s, best loss: 0.5574884716362115]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:04<00:00,  1.53s/it, best loss: 0.5735635415085881]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Amundson 2008\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:30<00:00,  3.85it/s, best loss: 0.5494082540148055]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:34<00:00,  3.57it/s, best loss: 0.5722808317834979]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:22<00:00,  2.53it/s, best loss: 0.5740818943776259]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:53<00:00,  1.64it/s, best loss: 0.5732746823331234]\n",
      "Split 5\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:35<00:00,  2.56it/s, best loss: 0.584521486171198]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:37<00:00,  2.55it/s, best loss: 0.5622199984676846]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:46<00:00,  2.04it/s, best loss: 0.5655805872958248]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:13<00:00,  4.63it/s, best loss: 0.6017748147481112]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:44<00:00,  2.90it/s, best loss: 0.5702391997582633]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:45<00:00,  2.83it/s, best loss: 0.5581289985097746]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:53<00:00,  2.85it/s, best loss: 0.5728786571588823]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:01<00:00,  4.69it/s, best loss: 0.5764425203436175]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:27<00:00,  2.60it/s, best loss: 0.5826925232851909]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:53<00:00,  2.22it/s, best loss: 0.5800156747444387]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:56<00:00,  3.26it/s, best loss: 0.5648417158474824]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:41<00:00,  3.05it/s, best loss: 0.5958829864340282]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:24<00:00,  2.68it/s, best loss: 0.5716524796769308]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:37<00:00,  3.47it/s, best loss: 0.5534686670151938]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:27<00:00,  3.07it/s, best loss: 0.5715718819625089]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:52<00:00,  2.11it/s, best loss: 0.5798260847275819]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Weichselbaum 2008\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:28<00:00,  3.29it/s, best loss: 0.561873551560536]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:52<00:00,  1.88it/s, best loss: 0.5766670215973517]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:45<00:00,  2.02it/s, best loss: 0.5661628847918545]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:14<00:00,  1.25it/s, best loss: 0.5985275102710169]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:06<00:00,  1.91it/s, best loss: 0.5653045282949882]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:47<00:00,  1.54it/s, best loss: 0.5727869740199243]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:21<00:00,  2.18it/s, best loss: 0.5829300602403958]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:42<00:00,  1.68it/s, best loss: 0.5839087530821945]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:08<00:00,  2.01it/s, best loss: 0.5889061707804821]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:43<00:00,  2.45it/s, best loss: 0.5793104622180342]\n",
      "Split 11\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:20<00:00,  2.80it/s, best loss: 0.566235296698387]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:48<00:00,  2.46it/s, best loss: 0.5876321330108547]\n",
      "Split 13\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:06<00:00,  5.31it/s, best loss: 0.584616005083088]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:38<00:00,  3.87it/s, best loss: 0.5895282388430368]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:27<00:00,  3.76it/s, best loss: 0.5717988422634582]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:58<00:00,  1.67it/s, best loss: 0.5925016837158592]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:12<00:00,  3.83it/s, best loss: 0.5714787796394142]\n",
      "Split 18\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:34<00:00,  3.78it/s, best loss: 0.577457737740185]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:13<00:00,  3.61it/s, best loss: 0.5684679584558846]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:01<00:00,  1.75it/s, best loss: 0.6032091561821267]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Piening 2009\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:13<00:00,  1.79it/s, best loss: 0.5325907731660791]\n",
      "Split 2\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:26<00:00,  1.51it/s, best loss: 0.549676849687576]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:03<00:00,  1.16it/s, best loss: 0.5499989361665373]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:50<00:00,  1.01it/s, best loss: 0.5614383034835257]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:47<00:00,  1.89it/s, best loss: 0.5575699188882662]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:01<00:00,  1.33it/s, best loss: 0.5540999736854927]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:37<00:00,  1.49it/s, best loss: 0.5450990249528689]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:01<00:00,  1.02it/s, best loss: 0.5800190408448916]\n",
      "Split 9\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:04<00:00,  1.48it/s, best loss: 0.556862779693663]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:10<00:00,  1.02it/s, best loss: 0.5352596785259185]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:48<00:00,  1.59it/s, best loss: 0.5621766603147488]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:13<00:00,  1.03s/it, best loss: 0.5716247490907239]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:26<00:00,  2.23it/s, best loss: 0.5737290993013278]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [04:19<00:00,  1.04s/it, best loss: 0.5677755171082053]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:36<00:00,  2.38it/s, best loss: 0.5489306587246824]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:50<00:00,  2.91it/s, best loss: 0.5621546927317921]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:57<00:00,  1.30it/s, best loss: 0.5428525592518594]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:39<00:00,  1.05it/s, best loss: 0.5476757981040287]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [03:07<00:00,  1.06it/s, best loss: 0.5479444538365119]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:54<00:00,  1.79it/s, best loss: 0.5598640837151058]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Kim 2012\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:51<00:00,  4.54it/s, best loss: 0.5462439455106718]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:37<00:00,  2.40it/s, best loss: 0.5694851391472906]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:15<00:00,  4.96it/s, best loss: 0.5593582879095346]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:40<00:00,  3.69it/s, best loss: 0.5681984556353564]\n",
      "Split 5\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:15<00:00,  1.52it/s, best loss: 0.571951346490203]\n",
      "Split 6\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:07<00:00,  5.41it/s, best loss: 0.565006458108987]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:45<00:00,  1.99it/s, best loss: 0.5758403890479137]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:08<00:00,  3.71it/s, best loss: 0.5806945107701996]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:21<00:00,  3.11it/s, best loss: 0.5560894878333256]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:40<00:00,  2.17it/s, best loss: 0.5485633374990062]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:29<00:00,  2.80it/s, best loss: 0.5668050679096905]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:39<00:00,  1.82it/s, best loss: 0.5888181963440409]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:24<00:00,  2.24it/s, best loss: 0.5751536301131615]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:35<00:00,  3.38it/s, best loss: 0.5778373745677068]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:39<00:00,  2.32it/s, best loss: 0.5560548494875833]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:35<00:00,  1.99it/s, best loss: 0.5764944819375826]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:38<00:00,  2.60it/s, best loss: 0.5609096506647558]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:18<00:00,  3.88it/s, best loss: 0.5500599575783991]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:18<00:00,  4.87it/s, best loss: 0.5535561788016118]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:55<00:00,  2.32it/s, best loss: 0.5703554323480806]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Abazeed 2013\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [13:35<00:00,  2.63s/it, best loss: 0.5161768441631999]\n",
      "Split 2\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [14:47<00:00,  2.75s/it, best loss: 0.546618662484048]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [15:16<00:00,  5.41s/it, best loss: 0.5430000861476365]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [25:12<00:00,  8.93s/it, best loss: 0.5537816475417929]\n",
      "Split 5\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [17:39<00:00,  5.21s/it, best loss: 0.554068645227717]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:09<00:00,  2.58s/it, best loss: 0.5450009209470091]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [26:15<00:00,  4.08s/it, best loss: 0.5539750940620983]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [16:32<00:00,  3.30s/it, best loss: 0.5662951543471655]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [14:24<00:00,  3.35s/it, best loss: 0.5462692097007101]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [22:32<00:00,  3.81s/it, best loss: 0.5196863535022082]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [18:14<00:00,  6.08s/it, best loss: 0.5417874938777958]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [15:33<00:00,  4.42s/it, best loss: 0.5519465430189616]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [18:09<00:00,  4.76s/it, best loss: 0.5639866932185501]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [22:42<00:00,  5.37s/it, best loss: 0.5543770623927893]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [18:52<00:00,  4.51s/it, best loss: 0.5422847365893101]\n",
      "Split 16\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [22:19<00:00,  4.77s/it, best loss: 0.557871792143536]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [16:32<00:00,  2.61s/it, best loss: 0.5508171525926651]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [15:36<00:00,  3.83s/it, best loss: 0.5354308292400186]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:34<00:00,  3.07s/it, best loss: 0.5344136434126376]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [20:50<00:00,  4.08s/it, best loss: 0.5518126656933996]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Pitroda 2014\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:06<00:00,  4.61it/s, best loss: 0.6286930570401139]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:57<00:00,  7.11it/s, best loss: 0.6449340409794869]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:42<00:00,  3.51it/s, best loss: 0.6528759424435491]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:15<00:00,  3.88it/s, best loss: 0.6508125927180669]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:50<00:00,  4.75it/s, best loss: 0.6500785802999143]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:18<00:00,  3.18it/s, best loss: 0.6399149130374775]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:14<00:00,  3.23it/s, best loss: 0.6523374136925256]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:56<00:00,  4.39it/s, best loss: 0.6572301362877775]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:55<00:00,  4.82it/s, best loss: 0.6505419327315551]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:49<00:00,  6.35it/s, best loss: 0.6473001943606561]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:19<00:00,  4.00it/s, best loss: 0.6477890288598244]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:01<00:00,  5.17it/s, best loss: 0.6593236858934955]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:37<00:00,  7.09it/s, best loss: 0.6421174262400458]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:13<00:00,  3.96it/s, best loss: 0.6534630001961603]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:51<00:00,  6.50it/s, best loss: 0.6325323308739446]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:12<00:00,  3.90it/s, best loss: 0.6575759013277136]\n",
      "Split 17\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:25<00:00,  2.38it/s, best loss: 0.650624232873789]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:24<00:00,  2.58it/s, best loss: 0.6394045754460027]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:34<00:00,  3.26it/s, best loss: 0.6422959211976513]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:45<00:00,  6.51it/s, best loss: 0.6434447409585327]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: de Jong 2015\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:16<00:00,  2.06s/it, best loss: 0.5292608777022738]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [07:39<00:00,  1.11s/it, best loss: 0.5384295464613151]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [07:52<00:00,  2.15s/it, best loss: 0.5408456813702146]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:22<00:00,  4.82s/it, best loss: 0.5535217523497054]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [14:17<00:00,  3.98s/it, best loss: 0.5531818060547835]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [07:43<00:00,  1.40s/it, best loss: 0.5506223481302612]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:13<00:00,  2.12s/it, best loss: 0.5481560359460961]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [08:18<00:00,  1.57s/it, best loss: 0.5795915858761221]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [13:09<00:00,  2.18s/it, best loss: 0.5493337233218111]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:21<00:00,  2.90s/it, best loss: 0.5283684172267656]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:29<00:00,  3.02s/it, best loss: 0.5424892861666808]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:39<00:00,  2.51s/it, best loss: 0.5653579666081597]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [09:42<00:00,  3.34s/it, best loss: 0.5557095909641693]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [13:52<00:00,  3.04s/it, best loss: 0.5525183714620209]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:24<00:00,  2.33s/it, best loss: 0.5477099867185453]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:27<00:00,  1.98s/it, best loss: 0.5648370345078362]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:47<00:00,  2.50s/it, best loss: 0.5440389533058622]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [09:50<00:00,  2.43s/it, best loss: 0.5396875712294175]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [09:49<00:00,  1.85s/it, best loss: 0.5462102608158305]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [13:11<00:00,  2.93s/it, best loss: 0.5533108633070575]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Speers 2015\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:08<00:00,  4.61it/s, best loss: 0.5242415762601567]\n",
      "Split 2\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:27<00:00,  3.66it/s, best loss: 0.556444875452438]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:25<00:00,  2.32it/s, best loss: 0.5511002039082933]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:43<00:00,  2.23it/s, best loss: 0.5516477817813703]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:49<00:00,  3.48it/s, best loss: 0.5615663257541103]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:56<00:00,  2.26it/s, best loss: 0.5492289938776986]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:19<00:00,  3.21it/s, best loss: 0.5481011840741743]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:37<00:00,  2.28it/s, best loss: 0.5552717794644254]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:42<00:00,  1.14it/s, best loss: 0.5550138268343564]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:08<00:00,  4.43it/s, best loss: 0.5263964713465136]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:06<00:00,  2.05it/s, best loss: 0.5559212824052208]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:08<00:00,  5.18it/s, best loss: 0.5574485066108024]\n",
      "Split 13\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:39<00:00,  2.17it/s, best loss: 0.575517994320631]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:00<00:00,  1.99it/s, best loss: 0.5585631139440187]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:13<00:00,  4.01it/s, best loss: 0.5222669407420832]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:51<00:00,  1.31it/s, best loss: 0.5640300632691779]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:31<00:00,  2.85it/s, best loss: 0.5431113437092352]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:29<00:00,  3.10it/s, best loss: 0.5376220881253657]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:18<00:00,  4.03it/s, best loss: 0.5316624168438212]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:48<00:00,  2.08it/s, best loss: 0.5550135011262072]\n",
      "-------------------------\n",
      "CLASSIFIER: GBM\n",
      "GENE LIST: Tang 2017\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:10<00:00,  5.20it/s, best loss: 0.5408111737947514]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:22<00:00,  2.14it/s, best loss: 0.5540372812349463]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:22<00:00,  3.23it/s, best loss: 0.5457448500580668]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:26<00:00,  3.51it/s, best loss: 0.5456438588541777]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [02:09<00:00,  1.19it/s, best loss: 0.5606035167597918]\n",
      "Split 6\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:22<00:00,  2.44it/s, best loss: 0.557293186338216]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:38<00:00,  1.91it/s, best loss: 0.5694648062546678]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:51<00:00,  1.99it/s, best loss: 0.5635136122254003]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:45<00:00,  2.00it/s, best loss: 0.5534818290498335]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:19<00:00,  4.64it/s, best loss: 0.5568363537110165]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:10<00:00,  6.06it/s, best loss: 0.5540985429611021]\n",
      "Split 12\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:27<00:00,  2.96it/s, best loss: 0.568650140068314]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:41<00:00,  2.38it/s, best loss: 0.5762396847082445]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:22<00:00,  3.44it/s, best loss: 0.5591565599128032]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:55<00:00,  1.72it/s, best loss: 0.5688420233383578]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  5.03it/s, best loss: 0.5594594934252329]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:25<00:00,  1.93it/s, best loss: 0.5486101139325616]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:04<00:00,  3.10it/s, best loss: 0.5585075299132713]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:41<00:00,  2.93it/s, best loss: 0.5490164261537339]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:13<00:00,  4.05it/s, best loss: 0.5528713493112759]\n"
     ]
    }
   ],
   "source": [
    "# iterate over classifiers\n",
    "for a in range(len(classifiers)):\n",
    "    \n",
    "    # output folder\n",
    "    if not os.path.isdir(classifiers[a]):\n",
    "        os.mkdir(classifiers[a])\n",
    "    \n",
    "    # performance files\n",
    "    performance_weightedlogloss = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=genelists)\n",
    "    performance_weightedlogloss.to_csv('%s/weightedlogloss.csv' % classifiers[a])\n",
    "\n",
    "    performance_balancedaccuracy = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=genelists)\n",
    "    performance_balancedaccuracy.to_csv('%s/balancedaccuracy.csv' % classifiers[a])\n",
    "\n",
    "    performance_auroc = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=genelists)\n",
    "    performance_auroc.to_csv('%s/auroc.csv' % classifiers[a])\n",
    "    \n",
    "    # iterate over gene lists\n",
    "    for b in range(len(genelists)):\n",
    "        print('-------------------------')\n",
    "        print('CLASSIFIER: %s' % classifiers[a])\n",
    "        print('GENE LIST: %s' % genelists[b])\n",
    "        print('-------------------------')\n",
    "\n",
    "        # load dataset\n",
    "        with open('../../_datasets/gene_all.pickle', 'rb') as f:\n",
    "            X_matrix, y_vector, _ = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        # subset features\n",
    "        X_matrix = X_matrix[genes[b]]\n",
    "        \n",
    "        # divide train+validation from testing\n",
    "        trainvalidation_index = []\n",
    "        test_index = []\n",
    "        sss = StratifiedShuffleSplit(n_splits=n_splits_trainvalidation_test, test_size=test_size, random_state=seed_)\n",
    "        for trainvalidation_, test_ in sss.split(X_matrix, y_vector):\n",
    "            trainvalidation_index.append(list(trainvalidation_))\n",
    "            test_index.append(list(test_))\n",
    "\n",
    "        # iterate over number of training+validation/testing splits\n",
    "        for c in range(n_splits_trainvalidation_test):\n",
    "            print('Split %d' % (c+1))\n",
    "\n",
    "            # separate train+validation and testing\n",
    "            X_trainvalidation = X_matrix.iloc[trainvalidation_index[c],]\n",
    "            X_test = X_matrix.iloc[test_index[c],]\n",
    "            y_trainvalidation = y_vector[trainvalidation_index[c]]\n",
    "            y_test = y_vector[test_index[c]]\n",
    "\n",
    "            # separate training_trainvalidation from earlystopping_trainvalidation\n",
    "            if classifiers[a] in ['GBM','NeuralNet']:\n",
    "                training_index = []\n",
    "                earlystopping_index = []\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "                for training_, earlystopping_ in sss.split(X_trainvalidation, y_trainvalidation):\n",
    "                    training_index.append(list(training_))\n",
    "                    earlystopping_index.append(list(earlystopping_))\n",
    "                X_training_trainvalidation = X_trainvalidation.iloc[training_index[0],]\n",
    "                X_earlystopping_trainvalidation = X_trainvalidation.iloc[earlystopping_index[0],]\n",
    "                y_training_trainvalidation = y_trainvalidation[training_index[0]]\n",
    "                y_earlystopping_trainvalidation = y_trainvalidation[earlystopping_index[0]]\n",
    "\n",
    "            # divide train from validation\n",
    "            train_index = []\n",
    "            validation_index = []\n",
    "            skf = StratifiedKFold(n_splits=k_train_validation, shuffle=True, random_state=seed_)\n",
    "            for train_, validation_ in skf.split(X_trainvalidation, y_trainvalidation):\n",
    "                train_index.append(list(train_))\n",
    "                validation_index.append(list(validation_))\n",
    "\n",
    "            # separate train and validation\n",
    "            X_train = []\n",
    "            X_validation = []\n",
    "            y_train = []\n",
    "            y_validation = []\n",
    "            for d in range(k_train_validation):\n",
    "                X_train.append(X_trainvalidation.iloc[train_index[d],])\n",
    "                X_validation.append(X_trainvalidation.iloc[validation_index[d],])\n",
    "                y_train.append(y_trainvalidation[train_index[d]])\n",
    "                y_validation.append(y_trainvalidation[validation_index[d]])\n",
    "\n",
    "            # separate training_train from earlystopping_train\n",
    "            if classifiers[a] in ['GBM','NeuralNet']:\n",
    "                X_training_train = []\n",
    "                X_earlystopping_train = []\n",
    "                y_training_train = []\n",
    "                y_earlystopping_train = []\n",
    "                for d in range(k_train_validation):\n",
    "                    training_index = []\n",
    "                    earlystopping_index = []\n",
    "                    sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "                    for training_, earlystopping_ in sss.split(X_train[d], y_train[d]):\n",
    "                        training_index.append(list(training_))\n",
    "                        earlystopping_index.append(list(earlystopping_))\n",
    "                    X_training_train.append(X_train[d].iloc[training_index[0],])\n",
    "                    X_earlystopping_train.append(X_train[d].iloc[earlystopping_index[0],])\n",
    "                    y_training_train.append(y_train[d][training_index[0]])\n",
    "                    y_earlystopping_train.append(y_train[d][earlystopping_index[0]])\n",
    "            \n",
    "            # imputation and scaling\n",
    "            if classifiers[a] in ['LogisticReg','SVM','NeuralNet']:\n",
    "                \n",
    "                # train+validation, testing\n",
    "                imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                X_trainvalidation = imp.fit_transform(X_trainvalidation)\n",
    "                X_test = imp.transform(X_test)\n",
    "                scaler = StandardScaler()\n",
    "                X_trainvalidation = scaler.fit_transform(X_trainvalidation)\n",
    "                X_test = scaler.transform(X_test)\n",
    "                \n",
    "                # training_trainvalidation, earlystopping_trainvalidation\n",
    "                if classifiers[a] in ['GBM','NeuralNet']:\n",
    "                    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                    X_training_trainvalidation = imp.fit_transform(X_training_trainvalidation)\n",
    "                    X_earlystopping_trainvalidation = imp.transform(X_earlystopping_trainvalidation)\n",
    "                    scaler = StandardScaler()\n",
    "                    X_training_trainvalidation = scaler.fit_transform(X_training_trainvalidation)\n",
    "                    X_earlystopping_trainvalidation = scaler.transform(X_earlystopping_trainvalidation)\n",
    "                    \n",
    "                # train, validation\n",
    "                for d in range(k_train_validation):\n",
    "                    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                    X_train[d] = imp.fit_transform(X_train[d])\n",
    "                    X_validation[d] = imp.transform(X_validation[d])\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train[d] = scaler.fit_transform(X_train[d])\n",
    "                    X_validation[d] = scaler.transform(X_validation[d])\n",
    "                    \n",
    "                # training_train, earlystopping_train\n",
    "                if classifiers[a] in ['GBM','NeuralNet']:\n",
    "                    for d in range(k_train_validation):\n",
    "                        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                        X_training_train[d] = imp.fit_transform(X_training_train[d])\n",
    "                        X_earlystopping_train[d] = imp.transform(X_earlystopping_train[d])\n",
    "                        scaler = StandardScaler()\n",
    "                        X_training_train[d] = scaler.fit_transform(X_training_train[d])\n",
    "                        X_earlystopping_train[d] = scaler.transform(X_earlystopping_train[d])\n",
    "\n",
    "            # hyperopt parameters\n",
    "            if classifiers[a] == 'LogisticReg':\n",
    "                parameters = {\n",
    "                    'C':hp.loguniform('C',np.log(1e-3),np.log(1e3)),\n",
    "                    'l1_ratio': hp.uniform('l1_ratio', 0, 1)\n",
    "                }\n",
    "            elif classifiers[a] == 'SVM':\n",
    "                parameters_choice = {\n",
    "                    'kernel':['linear','sigmoid','poly','rbf']\n",
    "                }\n",
    "                parameters = {\n",
    "                    'kernel':hp.choice('kernel', ['linear','sigmoid','poly','rbf']),\n",
    "                    'C':hp.loguniform('C',np.log(1e-2),np.log(1e2))\n",
    "                }\n",
    "            elif classifiers[a] == 'GBM':\n",
    "                parameters = {\n",
    "                    'gamma': hp.loguniform('gamma', np.log(0.0001), np.log(5)) - 0.0001,\n",
    "                    'max_depth': scope.int(hp.uniform('max_depth', 1, 11)),\n",
    "                    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "                    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n",
    "                    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(4)),\n",
    "                    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)) - 0.0001,\n",
    "                    'eta': hp.loguniform('eta', np.log(0.01), np.log(0.5))\n",
    "                }\n",
    "            elif classifiers[a] == 'NeuralNet':\n",
    "                parameters_choice = {\n",
    "                    'activation_function':['relu','elu','sigmoid'],\n",
    "                    'optimizer':['rmsprop', 'adam', 'sgd']\n",
    "                }\n",
    "                parameters = {\n",
    "                    'number_of_layers':scope.int(hp.quniform('number_of_layers',1.5,10.5,1)),\n",
    "                    'neurons_per_layer':scope.int(hp.qloguniform('neurons_per_layer',np.log(10.5),np.log(200.5),1)),\n",
    "                    'activation_function':hp.choice('activation_function', ['relu','elu','sigmoid']),\n",
    "                    'dropout_rate':hp.uniform('dropout_rate',0,0.5),\n",
    "                    'l1':hp.loguniform('l1', np.log(0.00001), np.log(0.001)),\n",
    "                    'l2':hp.loguniform('l2', np.log(0.001), np.log(1)),\n",
    "                    'optimizer':hp.choice('optimizer', ['rmsprop', 'adam', 'sgd'])\n",
    "                }\n",
    "\n",
    "            # save info for hyperopt\n",
    "            with open('_files/validation.pickle','wb') as f:\n",
    "                pickle.dump(1000., f)\n",
    "            with open('_files/data.pickle','wb') as f:\n",
    "                if classifiers[a] in ['GBM','NeuralNet']:\n",
    "                    pickle.dump([[X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train], X_validation, y_validation], f)\n",
    "                else:\n",
    "                    pickle.dump([[X_train, y_train], X_validation, y_validation], f)\n",
    "        \n",
    "            # hyperopt to find best parameters\n",
    "            trials = Trials()\n",
    "            best = fmin(hyperopt_function, parameters, algo=tpe.suggest, max_evals=n_hyperopt_iterations, trials=trials, rstate=np.random.RandomState(seed_), verbose=0, show_progressbar=True)\n",
    "\n",
    "            # create classifier using best parameters\n",
    "            if classifiers[a] in ['GBM','NeuralNet']:\n",
    "                pos_weight = len([x for x in y_training_trainvalidation if x==0])/len([x for x in y_training_trainvalidation if x==1])\n",
    "            else:\n",
    "                pos_weight = len([x for x in y_trainvalidation if x==0])/len([x for x in y_trainvalidation if x==1])\n",
    "            \n",
    "            # logistic regression\n",
    "            if classifiers[a] == 'LogisticReg':\n",
    "                \n",
    "                # parameters\n",
    "                parameters = {\n",
    "                    'C':best['C'],\n",
    "                    'l1_ratio':best['l1_ratio']\n",
    "                }\n",
    "                \n",
    "                # create classifier\n",
    "                clf = LogisticRegression(penalty='elasticnet', class_weight='balanced', solver='saga', max_iter=10000, random_state=seed_, **parameters)\n",
    "\n",
    "                # train on training+validation\n",
    "                clf.fit(X_trainvalidation, y_trainvalidation)\n",
    "\n",
    "                # evaluate on testing\n",
    "                y_pred = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "            # SVM\n",
    "            elif classifiers[a] == 'SVM':\n",
    "                \n",
    "                # parameters\n",
    "                parameters = {\n",
    "                    'kernel':parameters_choice['kernel'][best['kernel']],\n",
    "                    'C':best['C']\n",
    "                }\n",
    "                \n",
    "                # create classifier\n",
    "                clf = SVC(gamma='auto', probability=True, random_state=seed_, **parameters)\n",
    "\n",
    "                # train on training+validation\n",
    "                clf.fit(X_trainvalidation, y_trainvalidation)\n",
    "\n",
    "                # evaluate on testing\n",
    "                y_pred = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "            # GBM\n",
    "            elif classifiers[a] == 'GBM':\n",
    "                \n",
    "                # parameters\n",
    "                parameters = {\n",
    "                    'gamma': best['gamma'],\n",
    "                    'max_depth': int(np.round(best['max_depth'])),\n",
    "                    'subsample': best['subsample'],\n",
    "                    'colsample_bytree': best['colsample_bytree'],\n",
    "                    'colsample_bylevel': best['colsample_bylevel'],\n",
    "                    'reg_lambda': best['reg_lambda'],\n",
    "                    'reg_alpha': best['reg_alpha'],\n",
    "                    'eta':best['eta']\n",
    "                }\n",
    "                \n",
    "                # positive weight\n",
    "                pos_weight = len([x for x in y_training_trainvalidation if x==0])/len([x for x in y_training_trainvalidation if x==1])\n",
    "\n",
    "                # xgb datasets\n",
    "                xgb_training = xgb.DMatrix(X_training_trainvalidation, label=y_training_trainvalidation)\n",
    "                xgb_earlystopping = xgb.DMatrix(X_earlystopping_trainvalidation, label=y_earlystopping_trainvalidation)\n",
    "                xgb_testing = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "                # parameters\n",
    "                param = parameters.copy()\n",
    "                param['objective'] = 'binary:logistic'\n",
    "                param['eval_metric'] = 'logloss'\n",
    "                param['scale_pos_weight'] = pos_weight\n",
    "                param['seed'] = seed_\n",
    "                evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "                # train on training\n",
    "                bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "                # evaluate on validation\n",
    "                y_pred = bst.predict(xgb_testing, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "            # neural network\n",
    "            elif classifiers[a] == 'NeuralNet':\n",
    "                \n",
    "                # train best model on training+validation set\n",
    "                with tf.Graph().as_default():\n",
    "                    with tf.Session() as sess:\n",
    "\n",
    "                        # create model\n",
    "                        model = Sequential()\n",
    "                        for j in range(int(np.round(best['number_of_layers']))):\n",
    "                            model.add(Dense(int(np.round(best['neurons_per_layer'])), activation=parameters_choice['activation_function'][best['activation_function']], kernel_regularizer=regularizers.l1_l2(l1=best['l1'], l2=best['l2'])))\n",
    "                            model.add(Dropout(best['dropout_rate']))\n",
    "                        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                        # loss and performance metrics\n",
    "                        pos_weight = len([x for x in y_training_trainvalidation if x==0])/len([x for x in y_training_trainvalidation if x==1])\n",
    "                        model.compile(loss=weighted_cross_entropy(pos_weight), metrics=[weighted_cross_entropy(pos_weight)], optimizer=parameters_choice['optimizer'][best['optimizer']])      \n",
    "                        earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "                        # fit and evaluate model\n",
    "                        model.fit(X_training_trainvalidation, y_training_trainvalidation, epochs=1000, verbose=0, validation_data=(X_earlystopping_trainvalidation, y_earlystopping_trainvalidation), callbacks=[earlystopping])\n",
    "                        y_pred = model.predict_proba(X_test, batch_size=len(y_test), verbose=0)\n",
    "\n",
    "                # clear session\n",
    "                K.clear_session()\n",
    "\n",
    "            # calculate test performance - weighted log loss\n",
    "            pos_weight = len([x for x in y_test if x==0])/len([x for x in y_test if x==1])\n",
    "            sample_weights = [pos_weight if x==1 else 1 for x in y_test]\n",
    "            performance = log_loss(y_test, y_pred, sample_weight=sample_weights)\n",
    "            performance_weightedlogloss.at['split_%d' % (c+1), genelists[b]] = performance\n",
    "            performance_weightedlogloss.at['MEAN', genelists[b]] = np.nanmean(performance_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], genelists[b]].values.tolist())\n",
    "            performance_weightedlogloss.at['STERR', genelists[b]] = np.nanstd(performance_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], genelists[b]].values.tolist())/np.sqrt(c+1)\n",
    "            performance_weightedlogloss.to_csv('%s/weightedlogloss.csv' % classifiers[a])\n",
    "\n",
    "            # calculate test performance - balanced accuracy\n",
    "            y_pred_ = [1 if x>=0.5 else 0 for x in y_pred]\n",
    "            performance = balanced_accuracy_score(y_test, y_pred_)\n",
    "            performance_balancedaccuracy.at['split_%d' % (c+1), genelists[b]] = performance\n",
    "            performance_balancedaccuracy.at['MEAN', genelists[b]] = np.nanmean(performance_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], genelists[b]].values.tolist())\n",
    "            performance_balancedaccuracy.at['STERR', genelists[b]] = np.nanstd(performance_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], genelists[b]].values.tolist())/np.sqrt(c+1)\n",
    "            performance_balancedaccuracy.to_csv('%s/balancedaccuracy.csv' % classifiers[a])\n",
    "\n",
    "            # calculate test performance - auroc\n",
    "            y_pred_ = np.concatenate((np.array([1-x for x in y_pred]).reshape(-1,1), y_pred.reshape(-1,1)), axis=1)\n",
    "            performance = roc_auc_score(dummy_y(y_test), y_pred_)\n",
    "            performance_auroc.at['split_%d' % (c+1), genelists[b]] = performance\n",
    "            performance_auroc.at['MEAN', genelists[b]] = np.nanmean(performance_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], genelists[b]].values.tolist())\n",
    "            performance_auroc.at['STERR', genelists[b]] = np.nanstd(performance_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], genelists[b]].values.tolist())/np.sqrt(c+1)\n",
    "            performance_auroc.to_csv('%s/auroc.csv' % classifiers[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
