{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "from scipy.stats import ttest_ind, spearmanr\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder and datset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'withhyperopt_2'\n",
    "individual_datasets = ['gene_all']\n",
    "individual_datasets_labels = ['Gene Expression']\n",
    "colors = ['#3DAEC5']\n",
    "n_splits = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '_shap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_stacker = True\n",
    "analyze_shap_values = True\n",
    "analyze_shap_interactions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of dataset\n",
    "dataset_name = '+'.join(individual_datasets)\n",
    "\n",
    "# output folders\n",
    "if not os.path.isdir('%s/%s/%s' % (folder_name, dataset_name, output_folder)):\n",
    "    os.mkdir('%s/%s/%s' % (folder_name, dataset_name, output_folder))\n",
    "    \n",
    "    # stacker\n",
    "    if analyze_stacker:\n",
    "        os.mkdir('%s/%s/%s/stacker' % (folder_name, dataset_name, output_folder))\n",
    "        os.mkdir('%s/%s/%s/stacker/summary' % (folder_name, dataset_name, output_folder))\n",
    "        os.mkdir('%s/%s/%s/stacker/clinical_features' % (folder_name, dataset_name, output_folder))\n",
    "        \n",
    "    # shap values\n",
    "    if analyze_shap_values:\n",
    "        os.mkdir('%s/%s/%s/shap_values' % (folder_name, dataset_name, output_folder))\n",
    "        os.mkdir('%s/%s/%s/shap_values/summary' % (folder_name, dataset_name, output_folder))\n",
    "        os.mkdir('%s/%s/%s/shap_values/summary/cohort' % (folder_name, dataset_name, output_folder))\n",
    "        os.mkdir('%s/%s/%s/shap_values/features' % (folder_name, dataset_name, output_folder))\n",
    "        os.mkdir('%s/%s/%s/shap_values/patients' % (folder_name, dataset_name, output_folder))\n",
    "    \n",
    "    # shap interactions\n",
    "    if analyze_shap_interactions:\n",
    "        os.mkdir('%s/%s/%s/shap_interactions' % (folder_name, dataset_name, output_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create merged dataset\n",
    "with open('_datasets/%s.pickle' % individual_datasets[0], 'rb') as f:\n",
    "    X_matrix, y_vector, categorical_conversion_old = pickle.load(f, encoding='latin1')\n",
    "X_matrix.columns = ['%s # %s' % (individual_datasets[0], x) for x in X_matrix.columns.tolist()]\n",
    "categorical_conversion = {}\n",
    "for key in categorical_conversion_old:\n",
    "    categorical_conversion['%s # %s' % (individual_datasets[0], key)] = categorical_conversion_old[key]\n",
    "for b in range(1,len(individual_datasets)):\n",
    "    with open('_datasets/%s.pickle' % individual_datasets[b], 'rb') as f:\n",
    "        X_matrix_, y_vector_, categorical_conversion_old = pickle.load(f, encoding='latin1')\n",
    "    X_matrix_.columns = ['%s # %s' % (individual_datasets[b], x) for x in X_matrix_.columns.tolist()]\n",
    "    categorical_conversion_ = {}\n",
    "    for key in categorical_conversion_old:\n",
    "        categorical_conversion_['%s # %s' % (individual_datasets[b], key)] = categorical_conversion_old[key]\n",
    "    if X_matrix.index.tolist() == X_matrix_.index.tolist():\n",
    "        X_matrix = pd.concat([X_matrix, X_matrix_], axis=1)\n",
    "        categorical_conversion = {**categorical_conversion, **categorical_conversion_}\n",
    "    else:\n",
    "        raise Exception(\"Dataset sample lists don't match\")\n",
    "samples = X_matrix.index.tolist()\n",
    "features =  X_matrix.columns.tolist()\n",
    "\n",
    "# list of merged features\n",
    "if len(categorical_conversion) > 0:\n",
    "    merged_features = []\n",
    "    for feature in features:\n",
    "        if feature.split(' | ')[0] not in categorical_conversion:\n",
    "            merged_features.append(feature)\n",
    "        elif feature.split(' | ')[0] not in merged_features:\n",
    "            merged_features.append(feature.split(' | ')[0])\n",
    "else:\n",
    "    merged_features = features.copy()\n",
    "    \n",
    "# merge feature matrix\n",
    "X_matrix_ = X_matrix.copy()\n",
    "for feature in categorical_conversion:\n",
    "    X_matrix_[feature] = np.nan\n",
    "    X_matrix_[feature] = X_matrix_[feature].astype(object)\n",
    "    \n",
    "    # get values for categorical features\n",
    "    values = []\n",
    "    for sample in X_matrix.index.tolist():\n",
    "        for val in categorical_conversion[feature]:\n",
    "            if X_matrix.at[sample,'%s | %s' % (feature, val)] == 1:\n",
    "                X_matrix_.at[sample, feature] = val\n",
    "X_matrix = X_matrix_[merged_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier - weighted log loss\n",
    "df = pd.read_csv('%s/weightedlogloss.csv' % folder_name, index_col=0)\n",
    "classifier_weighted_log_loss = df.loc[['split_%d' % (i+1) for i in range(n_splits)]][dataset_name].values.tolist()\n",
    "classifier_performance_weights = [1/x for x in classifier_weighted_log_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_shap_values:\n",
    "        \n",
    "    # load shap values\n",
    "    shap_expected = []\n",
    "    shap_values = []\n",
    "    for i in range(n_splits):\n",
    "        with open('%s/%s/shap_values_%d.pickle' % (folder_name, dataset_name, i+1) ,'rb') as f:\n",
    "            shap_expected_, shap_values_ = pickle.load(f)\n",
    "            shap_expected.append(shap_expected_)\n",
    "            shap_values.append(shap_values_)\n",
    "            \n",
    "    # remove samples that aren't analyzed\n",
    "    keep_index = []\n",
    "    for i,sample in enumerate(samples):\n",
    "        to_keep = False\n",
    "        for j in range(n_splits):\n",
    "            if sample in shap_values[j].index.tolist():\n",
    "                to_keep = True\n",
    "        if to_keep:\n",
    "            keep_index.append(i)\n",
    "    samples = [samples[i] for i in keep_index]\n",
    "    y_vector = [y_vector[i] for i in keep_index]\n",
    "    X_matrix = X_matrix.iloc[keep_index]\n",
    "    \n",
    "    # combine expected values\n",
    "    _expected_value_sum = [0 for sample in samples]\n",
    "    _shap_weight = [0 for sample in samples]\n",
    "    for i,sample in enumerate(samples):\n",
    "        for j in range(n_splits):\n",
    "            if sample in shap_values[j].index.tolist():\n",
    "                _expected_value_sum[i] += classifier_performance_weights[j] * shap_expected[j][shap_values[j].index.tolist().index(sample)]\n",
    "                _shap_weight[i] += classifier_performance_weights[j]\n",
    "    expected_value = np.divide(_expected_value_sum, _shap_weight)\n",
    "    \n",
    "    # combine shap values\n",
    "    for i in range(n_splits):\n",
    "        samples_not_included = [x for x in samples if x not in shap_values[i].index.tolist()]\n",
    "        shap_values[i] = pd.concat([shap_values[i], pd.DataFrame(data=0, index=samples_not_included, columns=shap_values[i].columns.tolist())])\n",
    "        shap_values[i] = shap_values[i].loc[samples]\n",
    "        features_not_included = [x for x in merged_features if x not in shap_values[i].columns.tolist()]\n",
    "        shap_values[i] = pd.concat([shap_values[i], pd.DataFrame(data=0, index=samples, columns=features_not_included)], axis=1, sort=False)\n",
    "        shap_values[i] = shap_values[i][merged_features]\n",
    "    shap_value = pd.DataFrame(data=0, index=samples, columns=merged_features)\n",
    "    for i in range(n_splits):\n",
    "        shap_value = np.add(shap_value, classifier_performance_weights[i]*shap_values[i])\n",
    "    for j in range(len(samples)):\n",
    "        shap_value.loc[samples[j]] /= _shap_weight[j]\n",
    "    \n",
    "    # calculate absolute shap values\n",
    "    shap_value_abs = shap_value.abs()\n",
    "    \n",
    "    # zero shap values for imputed features\n",
    "    for sample in shap_value_abs.index.tolist():\n",
    "        for feature in shap_value_abs.columns.tolist():\n",
    "            if pd.isna(X_matrix.at[sample,feature]):\n",
    "                shap_value_abs.at[sample,feature] = 0\n",
    "    \n",
    "    # normalize by difference between prior and posterior\n",
    "    for i,sample in enumerate(samples):\n",
    "        shap_value_abs.loc[sample] /= np.sum(shap_value_abs.loc[sample].values.tolist())\n",
    "        \n",
    "    # mean and standard error for each feature\n",
    "    shap_value_abs_mean = shap_value_abs.mean(axis=0).values.tolist()\n",
    "    shap_value_abs_mean_cumsum = np.cumsum(sorted(shap_value_abs_mean)[::-1])\n",
    "    shap_value_abs_std = shap_value_abs.std(axis=0).values.tolist()\n",
    "    shap_value_abs_sterr = [x/np.sqrt(len(samples)) for x in shap_value_abs_std]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_shap_values:\n",
    "    \n",
    "    # genes with evidence\n",
    "    evidence = ['CDH10','TSC22D4','SEMA5B','SCN7A','PTCHD1','OLIG1','HIF3A','IL19','BNIP3','WIPI1']\n",
    "    \n",
    "    # top N features - mean\n",
    "    top_N_index = np.argsort(shap_value_abs_mean)[::-1]\n",
    "    top_N_mean = [shap_value_abs_mean[i] for i in top_N_index]\n",
    "    top_N_sterr = [shap_value_abs_sterr[i] for i in top_N_index]\n",
    "    top_N_datasets = [merged_features[i].split(' # ')[0] for i in top_N_index]\n",
    "    top_N_features = [merged_features[i].split(' # ')[-1] for i in top_N_index]\n",
    "    \n",
    "    # plot - all shap values\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rc('xtick', labelsize=18)\n",
    "    plt.rc('ytick', labelsize=18)\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xscale('log')\n",
    "    ax.plot(range(1,len(merged_features)+1), sorted(shap_value_abs_mean)[::-1], 'k.-', markersize=5)\n",
    "    for i in range(len(evidence)):\n",
    "        ax.plot(top_N_features.index(evidence[i])+1, top_N_mean[top_N_features.index(evidence[i])], '.', markersize=5, color=colors[0])\n",
    "    plt.ylabel(r'Mean |$\\Delta$P|', fontsize=20)\n",
    "    plt.ylim(-0.0001,0.03)\n",
    "    plt.yticks([0,0.01,0.02,0.03])\n",
    "    plt.xlabel('Features', fontsize=20)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(range(1,len(merged_features)+1), shap_value_abs_mean_cumsum, '.-', color='#808080', markersize=5)\n",
    "    ax2.plot([len([x for x in shap_value_abs_mean_cumsum if x<=0.95]),len([x for x in shap_value_abs_mean_cumsum if x<=0.95])],[0,0.95],'--',color='#808080')\n",
    "    ax2.plot([len([x for x in shap_value_abs_mean_cumsum if x<=0.95]),1.1*len(merged_features)],[0.95,0.95],'--',color='#808080')\n",
    "    ax2.plot([len([x for x in shap_value_abs_mean if x>0]),len([x for x in shap_value_abs_mean if x>0])],[0,1],'--',color='#808080')\n",
    "    ax2.tick_params(axis='y', labelcolor='#808080')\n",
    "    ax2.set_ylabel('Cumulative Sum', color='#808080', fontsize=20)\n",
    "    ax2.set_ylim(0,1.01)\n",
    "    ax2.set_yticks([0,0.25,0.5,0.75,0.95,1])\n",
    "    num_int = len(str(len(merged_features)))\n",
    "    plt.xticks([10**x for x in range(num_int)][:-1]+[len(merged_features)],[10**x for x in range(num_int)][:-1]+[len(merged_features)])\n",
    "    plt.xlim(0.95,1.05*len(merged_features))  \n",
    "    plt.text(len([x for x in shap_value_abs_mean_cumsum if x<=0.95]),0.03,'%d '% len([x for x in shap_value_abs_mean_cumsum if x<=0.95]), weight='bold', ha='right', fontsize=20)\n",
    "    plt.text(len([x for x in shap_value_abs_mean if x>0]),0.03,'%d' % len([x for x in shap_value_abs_mean if x>0]), ha='left', fontsize=20)\n",
    "    #plt.text(len(merged_features),0.03,'%d' % len(merged_features), weight='bold', ha='right', fontsize=20)\n",
    "    vals = ax.get_yticks()\n",
    "    decimals = [str(round(x*100,6)).split('.')[1] for x in vals]\n",
    "    if len([x for x in decimals if x != '0']) > 0:\n",
    "        number_of_places = np.max([len(x) for x in decimals])\n",
    "    else:\n",
    "        number_of_places = np.max([len(x) for x in decimals])-1\n",
    "    if number_of_places == 0:\n",
    "        ax.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\n",
    "    elif number_of_places == 1:\n",
    "        ax.set_yticklabels(['{:,.1%}'.format(x) for x in vals])\n",
    "    elif number_of_places == 2:\n",
    "        ax.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "    else:\n",
    "        raise Exception('Number of decimal places = %d' % number_of_places)\n",
    "    vals = ax2.get_yticks()\n",
    "    decimals = [str(round(x*100,6)).split('.')[1] for x in vals]\n",
    "    if len([x for x in decimals if x != '0']) > 0:\n",
    "        number_of_places = np.max([len(x) for x in decimals])\n",
    "    else:\n",
    "        number_of_places = np.max([len(x) for x in decimals])-1\n",
    "    if number_of_places == 0:\n",
    "        ax2.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\n",
    "    elif number_of_places == 1:\n",
    "        ax2.set_yticklabels(['{:,.1%}'.format(x) for x in vals])\n",
    "    elif number_of_places == 2:\n",
    "        ax2.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "    else:\n",
    "        raise Exception('Number of decimal places = %d' % number_of_places)\n",
    "    plt.savefig('%s/%s/%s/shap_values/summary/all_features.png' % (folder_name, dataset_name, output_folder), bbox_inches='tight', dpi=400)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene resulting in increased or decreased probability with increased expression\n",
    "increase = []\n",
    "decrease = []\n",
    "for feature in overall_top_features:\n",
    "    \n",
    "    # expression and shap values\n",
    "    expression = X_matrix['gene_all # %s' % feature].values.tolist()\n",
    "    shapvalue = shap_value['gene_all # %s' % feature].values.tolist()\n",
    "    keep_index = [i for i in range(len(expression)) if (not pd.isna(expression[i])) and (not pd.isna(shapvalue[i]))]\n",
    "    expression = [expression[i] for i in keep_index]\n",
    "    shapvalue = [shapvalue[i] for i in keep_index]\n",
    "    \n",
    "    # correlation coefficient\n",
    "    r = spearmanr(expression, shapvalue).correlation\n",
    "    if r > 0:\n",
    "        increase.append(feature)\n",
    "    else:\n",
    "        decrease.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrez gene lists for gene ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_shap_values:\n",
    "\n",
    "    # gene lists for gene ontology - significant genes\n",
    "    N = len([x for x in shap_value_abs_mean_cumsum if x<=0.95])\n",
    "    top_N_index = np.argsort(shap_value_abs_mean)[::-1][:N]\n",
    "    top_N_features = [merged_features[i].split(' # ')[-1] for i in top_N_index]\n",
    "    overall_top_features = top_N_features.copy()\n",
    "    found_entrez = []\n",
    "    stepsize = 400\n",
    "    for i in range(int(np.ceil(len(top_N_features)/stepsize))):\n",
    "        time.sleep(1)\n",
    "        if i < len(top_N_features)/stepsize:\n",
    "            gene_list = ','.join(top_N_features[(stepsize*i):(stepsize*(i+1))])\n",
    "        else:\n",
    "            gene_list = ','.join(top_N_features[(stepsize*i):])\n",
    "        link = 'https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=genesymbolandsynonyms&inputValues=%s&outputs=geneid&taxonId=9606&format=row' % gene_list\n",
    "        data = json.loads(urllib.request.urlopen(link).read())  \n",
    "        for j in range(len(data)):\n",
    "            if (data[j]['Gene ID'] != '-') and ('//' not in data[j]['Gene ID']):\n",
    "                found_entrez.append(str(data[j]['Gene ID']))\n",
    "    with open('%s/%s/%s/shap_values/summary/sig_genes.txt' % (folder_name, dataset_name, output_folder),'w') as f:\n",
    "        for gene in sorted(list(set(found_entrez))):\n",
    "            f.write('%s\\n' % gene)\n",
    "            \n",
    "    # gene lists for gene ontology - significant increase genes\n",
    "    found_entrez = []\n",
    "    stepsize = 400\n",
    "    for i in range(int(np.ceil(len(increase)/stepsize))):\n",
    "        time.sleep(1)\n",
    "        if i < len(increase)/stepsize:\n",
    "            gene_list = ','.join(increase[(stepsize*i):(stepsize*(i+1))])\n",
    "        else:\n",
    "            gene_list = ','.join(increase[(stepsize*i):])\n",
    "        link = 'https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=genesymbolandsynonyms&inputValues=%s&outputs=geneid&taxonId=9606&format=row' % gene_list\n",
    "        data = json.loads(urllib.request.urlopen(link).read())  \n",
    "        for j in range(len(data)):\n",
    "            if (data[j]['Gene ID'] != '-') and ('//' not in data[j]['Gene ID']):\n",
    "                found_entrez.append(str(data[j]['Gene ID']))\n",
    "    with open('%s/%s/%s/shap_values/summary/sig_genes_increase.txt' % (folder_name, dataset_name, output_folder),'w') as f:\n",
    "        for gene in sorted(list(set(found_entrez))):\n",
    "            f.write('%s\\n' % gene)\n",
    "            \n",
    "    # gene lists for gene ontology - significant increase genes\n",
    "    found_entrez = []\n",
    "    stepsize = 400\n",
    "    for i in range(int(np.ceil(len(decrease)/stepsize))):\n",
    "        time.sleep(1)\n",
    "        if i < len(decrease)/stepsize:\n",
    "            gene_list = ','.join(decrease[(stepsize*i):(stepsize*(i+1))])\n",
    "        else:\n",
    "            gene_list = ','.join(decrease[(stepsize*i):])\n",
    "        link = 'https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=genesymbolandsynonyms&inputValues=%s&outputs=geneid&taxonId=9606&format=row' % gene_list\n",
    "        data = json.loads(urllib.request.urlopen(link).read())  \n",
    "        for j in range(len(data)):\n",
    "            if (data[j]['Gene ID'] != '-') and ('//' not in data[j]['Gene ID']):\n",
    "                found_entrez.append(str(data[j]['Gene ID']))\n",
    "    with open('%s/%s/%s/shap_values/summary/sig_genes_decrease.txt' % (folder_name, dataset_name, output_folder),'w') as f:\n",
    "        for gene in sorted(list(set(found_entrez))):\n",
    "            f.write('%s\\n' % gene)\n",
    "            \n",
    "    # gene lists for gene ontology - reference genes\n",
    "    N = len(merged_features)\n",
    "    top_N_index = np.argsort(shap_value_abs_mean)[::-1][:N]\n",
    "    top_N_features = [merged_features[i].split(' # ')[-1] for i in top_N_index]\n",
    "    found_entrez = []\n",
    "    stepsize = 400\n",
    "    for i in range(int(np.ceil(len(top_N_features)/stepsize))):\n",
    "        time.sleep(1)\n",
    "        if i < len(top_N_features)/stepsize:\n",
    "            gene_list = ','.join(top_N_features[(stepsize*i):(stepsize*(i+1))])\n",
    "        else:\n",
    "            gene_list = ','.join(top_N_features[(stepsize*i):])\n",
    "        link = 'https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=genesymbolandsynonyms&inputValues=%s&outputs=geneid&taxonId=9606&format=row' % gene_list\n",
    "        data = json.loads(urllib.request.urlopen(link).read())  \n",
    "        for j in range(len(data)):\n",
    "            if (data[j]['Gene ID'] != '-') and ('//' not in data[j]['Gene ID']):\n",
    "                found_entrez.append(str(data[j]['Gene ID']))\n",
    "    with open('%s/%s/%s/shap_values/summary/ref_genes.txt' % (folder_name, dataset_name, output_folder),'w') as f:\n",
    "        for gene in sorted(list(set(found_entrez))):\n",
    "            f.write('%s\\n' % gene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_shap_values:\n",
    "    \n",
    "    # load cohort for each sample\n",
    "    with open('_datasets/clinical.pickle', 'rb') as f:\n",
    "        X_matrix_, y_vector_, categorical_conversion_ = pickle.load(f, encoding='latin1')\n",
    "    cohorts = []\n",
    "    for sample in X_matrix_.index.tolist():\n",
    "        for cohort in categorical_conversion_['COHORT']:\n",
    "            if X_matrix_.at[sample,'COHORT | %s' % cohort] == 1:\n",
    "                cohorts.append(cohort)\n",
    "\n",
    "    # iterate over datasets\n",
    "    for cohort in ['ACC', 'BLCA', 'BRCA', 'CESC', 'COAD', 'DLBC', 'GBM', 'HNSC', 'LGG', 'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'SKCM', 'STAD', 'THCA', 'UCEC', 'UCS']:\n",
    "        \n",
    "        # subset patients within cohort\n",
    "        samples_ = [samples[i] for i in range(len(samples)) if cohorts[i]==cohort]\n",
    "        shap_value_abs_ = shap_value_abs.loc[samples_]\n",
    "        \n",
    "        # mean and standard error for each feature\n",
    "        shap_value_abs_mean = shap_value_abs_.mean(axis=0).values.tolist()\n",
    "        shap_value_abs_mean_cumsum = np.cumsum(sorted(shap_value_abs_mean)[::-1])\n",
    "        shap_value_abs_std = shap_value_abs_.std(axis=0).values.tolist()\n",
    "        shap_value_abs_sterr = [x/np.sqrt(len(samples)) for x in shap_value_abs_std]\n",
    "        \n",
    "        # top N features - mean\n",
    "        N = len([x for x in shap_value_abs_mean_cumsum if x<=0.95])\n",
    "        top_N_index = np.argsort(shap_value_abs_mean)[::-1][:N]\n",
    "        top_N_mean = [shap_value_abs_mean[i] for i in top_N_index]\n",
    "        top_N_sterr = [shap_value_abs_sterr[i] for i in top_N_index]\n",
    "        top_N_datasets = [merged_features[i].split(' # ')[0] for i in top_N_index]\n",
    "        top_N_features = [merged_features[i].split(' # ')[-1] for i in top_N_index]\n",
    "        \n",
    "        # gene lists for gene ontology - significant genes\n",
    "        found_entrez = []\n",
    "        stepsize = 400\n",
    "        for i in range(int(np.ceil(len(top_N_features)/stepsize))):\n",
    "            time.sleep(1)\n",
    "            if i < len(top_N_features)/stepsize:\n",
    "                gene_list = ','.join(top_N_features[(stepsize*i):(stepsize*(i+1))])\n",
    "            else:\n",
    "                gene_list = ','.join(top_N_features[(stepsize*i):])\n",
    "            link = 'https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=genesymbolandsynonyms&inputValues=%s&outputs=geneid&taxonId=9606&format=row' % gene_list\n",
    "            data = json.loads(urllib.request.urlopen(link).read())  \n",
    "            for j in range(len(data)):\n",
    "                if (data[j]['Gene ID'] != '-') and ('//' not in data[j]['Gene ID']):\n",
    "                    found_entrez.append(str(data[j]['Gene ID']))\n",
    "        with open('%s/%s/%s/shap_values/summary/cohort/%s_sig_genes.txt' % (folder_name, dataset_name, output_folder, cohort),'w') as f:\n",
    "            for gene in sorted(list(set(found_entrez))):\n",
    "                f.write('%s\\n' % gene) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_shap_values:\n",
    "\n",
    "    # iterate over patients\n",
    "    for sample in samples:\n",
    "        \n",
    "        # get features with non-negative shap_values\n",
    "        sig_features = [feature for feature in merged_features if shap_value_abs.at[sample,feature]>0]\n",
    "        sig_values = [shap_value_abs.at[sample,x] for x in sig_features]\n",
    "        sort_index = np.argsort(sig_values)[::-1]\n",
    "        sig_features = [sig_features[i] for i in sort_index]\n",
    "        sig_values = [sig_values[i] for i in sort_index]\n",
    "        sig_values_cumsum = np.cumsum(sig_values)\n",
    "        sig_values_cumsum /= np.max(sig_values_cumsum)\n",
    "        \n",
    "        # top N features - mean\n",
    "        N = len([x for x in sig_values_cumsum if x<=0.95])\n",
    "        top_N_features = [x.split(' # ')[-1] for x in sig_features[:N]]\n",
    "        \n",
    "        # gene lists for gene ontology - significant genes\n",
    "        found_entrez = []\n",
    "        stepsize = 400\n",
    "        for i in range(int(np.ceil(len(top_N_features)/stepsize))):\n",
    "            time.sleep(1)\n",
    "            if i < len(top_N_features)/stepsize:\n",
    "                gene_list = ','.join(top_N_features[(stepsize*i):(stepsize*(i+1))])\n",
    "            else:\n",
    "                gene_list = ','.join(top_N_features[(stepsize*i):])\n",
    "            link = 'https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=genesymbolandsynonyms&inputValues=%s&outputs=geneid&taxonId=9606&format=row' % gene_list\n",
    "            data = json.loads(urllib.request.urlopen(link).read())  \n",
    "            for j in range(len(data)):\n",
    "                if (data[j]['Gene ID'] != '-') and ('//' not in data[j]['Gene ID']):\n",
    "                    found_entrez.append(str(data[j]['Gene ID']))\n",
    "        with open('%s/%s/%s/shap_values/summary/patient/%s_sig_genes.txt' % (folder_name, dataset_name, output_folder, sample),'w') as f:\n",
    "            for gene in sorted(list(set(found_entrez))):\n",
    "                f.write('%s\\n' % gene) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patient shap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "def plot_patient_example(patient_name, actual_value, expected_value, features_, shap_, original_):\n",
    "    \n",
    "    # initialize figure\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    fig = plt.figure(figsize=(20,.15*len(features_)))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # patient name\n",
    "    if actual_value == 0:\n",
    "        plt.text(0.5,0.8,patient_name, horizontalalignment='center', verticalalignment='center', color='#5bc2ae', fontsize=18, weight='bold')\n",
    "    else:\n",
    "        plt.text(0.5,1,patient_name, horizontalalignment='center', verticalalignment='center', color='#d93f20', fontsize=18, weight='bold')\n",
    "    \n",
    "    # number line\n",
    "    plt.plot([0.18,0.23],[0,0],'k-',linewidth=1)\n",
    "    plt.plot([0.25,1],[0,0],'k-',linewidth=1)\n",
    "    plt.plot([0.22,0.24],[-0.3,0.3],'k-',linewidth=1)\n",
    "    plt.plot([0.24,0.26],[-0.3,0.3],'k-',linewidth=1)\n",
    "    for x in [0.18,0.5,1]:\n",
    "        plt.plot([x,x],[-0.1,0.1],'k-',linewidth=1)\n",
    "    plt.plot([expected_value,expected_value],[-0.3,0.1],'k--',linewidth=1)\n",
    "    plt.text(0.18,0.1,'0%', horizontalalignment='center', verticalalignment='bottom', color='#5bc2ae', fontsize=16)\n",
    "    plt.text(0.5,0.1,'50%', horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "    plt.text(1,0.1,'100%', horizontalalignment='center', verticalalignment='bottom', color='#d93f20', fontsize=16)\n",
    "    plt.text(expected_value,0.1,'%0.1f%%' % (expected_value*100), horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "    plt.text(expected_value,0.75,'Prior', horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "    \n",
    "    # data\n",
    "    current_value = expected_value\n",
    "    y_value = -0.3\n",
    "    for i in range(len(features_)):\n",
    "        if not pd.isna(original_[i]):\n",
    "            if shap_[i] < 0:\n",
    "                plt.arrow(current_value, y_value, shap_[i], 0, width=0.05, head_length= 0.2*np.abs(shap_[i]), length_includes_head=True, color='#5bc2ae')\n",
    "                if type(original_[i]) == str:\n",
    "                    plt.text(current_value+0.005, y_value, '%s = %s' % (features_[i], original_[i]), horizontalalignment='left', verticalalignment='center', fontsize=10)\n",
    "                else:\n",
    "                    plt.text(current_value+0.005, y_value, '%s (Imputed)' % features_[i], horizontalalignment='left', verticalalignment='center', fontsize=10)\n",
    "            else:\n",
    "                plt.arrow(current_value, y_value, shap_[i], 0, width=0.05, head_length= 0.2*np.abs(shap_[i]), length_includes_head=True, color='#d93f20')\n",
    "                if type(original_[i]) == str:\n",
    "                    plt.text(current_value-0.005, y_value, '%s = %s' % (features_[i], original_[i]), horizontalalignment='right', verticalalignment='center', fontsize=10)\n",
    "                else:\n",
    "                    plt.text(current_value-0.005, y_value, '%s (Imputed)' % features_[i], horizontalalignment='right', verticalalignment='center', fontsize=10)\n",
    "            current_value += shap_[i]\n",
    "            y_value += -0.3\n",
    "        \n",
    "    # end line\n",
    "    plt.plot([current_value,current_value],[y_value+0.15,0.1],'k--',linewidth=1)\n",
    "    if current_value < 0.5:\n",
    "        plt.text(current_value,0.1,'%0.1f%%' % (current_value*100), horizontalalignment='center', verticalalignment='bottom', color='#5bc2ae', fontsize=16, weight='bold')\n",
    "    else:\n",
    "        plt.text(current_value,0.1,'%0.1f%%' % (current_value*100), horizontalalignment='center', verticalalignment='bottom', color='#d93f20', fontsize=16, weight='bold')\n",
    "    plt.text(current_value,0.75,'Posterior', horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "        \n",
    "    # limits\n",
    "    plt.xlim(-0.02,1.02)\n",
    "    plt.ylim(y_value-0.05, 0.8)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: TCGA-DU-8165-01A\n",
    "for a,sample in enumerate(samples):\n",
    "    if sample=='TCGA-DU-8165-01A':\n",
    "    \n",
    "        # get nonzero shap and feature values\n",
    "        features_ = [feature.split(' # ')[-1] for feature in merged_features if shap_value.at[sample,feature] != 0]\n",
    "        shap_ = shap_value.loc[sample][['gene_all # %s' % x for x in features_]].tolist()\n",
    "        original_ = X_matrix.loc[sample][['gene_all # %s' % x for x in features_]].tolist()\n",
    "\n",
    "        # convert numerical values to strings\n",
    "        for j in range(len(original_)):\n",
    "            if not type(original_[j]) == str:\n",
    "                if type(original_[j]) == bool:\n",
    "                    original_[j] = str(original_[j])\n",
    "                elif not np.isnan(original_[j]):\n",
    "                    if original_[j] == int(original_[j]):\n",
    "                        original_[j] = str(int(original_[j]))\n",
    "                    else:\n",
    "                        original_[j] = str(np.round(original_[j],2))+' TPM'\n",
    "\n",
    "        # order features based on absolute shap value\n",
    "        sort_index = np.argsort(np.abs(shap_))[::-1]\n",
    "        features_ = [features_[i] for i in sort_index]\n",
    "        shap_ = [shap_[i] for i in sort_index]\n",
    "        original_ = [original_[i] for i in sort_index]\n",
    "\n",
    "        # create figure\n",
    "        plot_patient_example(sample, y_vector[a], expected_value[a], features_, shap_, original_)\n",
    "        plt.savefig('%s/%s/%s/shap_values/patient_example.png' % (folder_name, dataset_name, output_folder), bbox_inches='tight', dpi=400)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "def plot_patient(patient_name, actual_value, expected_value, features_, shap_, original_):\n",
    "    \n",
    "    # initialize figure\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    fig = plt.figure(figsize=(20,.15*len(features_)))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # patient name\n",
    "    if actual_value == 0:\n",
    "        plt.text(0.5,0.8,patient_name, horizontalalignment='center', verticalalignment='center', color='#5bc2ae', fontsize=18, weight='bold')\n",
    "    else:\n",
    "        plt.text(0.5,1,patient_name, horizontalalignment='center', verticalalignment='center', color='#d93f20', fontsize=18, weight='bold')\n",
    "    \n",
    "    # number line\n",
    "    plt.plot([0,1],[0,0],'k-',linewidth=1)\n",
    "    for x in [0,0.5,1]:\n",
    "        plt.plot([x,x],[-0.1,0.1],'k-',linewidth=1)\n",
    "    plt.plot([expected_value,expected_value],[-0.3,0.1],'k--',linewidth=1)\n",
    "    plt.text(0,0.1,'0%', horizontalalignment='center', verticalalignment='bottom', color='#5bc2ae', fontsize=16)\n",
    "    plt.text(0.5,0.1,'50%', horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "    plt.text(1,0.1,'100%', horizontalalignment='center', verticalalignment='bottom', color='#d93f20', fontsize=16)\n",
    "    plt.text(expected_value,0.1,'%0.1f%%' % (expected_value*100), horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "    plt.text(expected_value,0.75,'Prior', horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "    \n",
    "    # data\n",
    "    current_value = expected_value\n",
    "    y_value = -0.3\n",
    "    for i in range(len(features_)):\n",
    "        if not pd.isna(original_[i]):\n",
    "            if shap_[i] < 0:\n",
    "                plt.arrow(current_value, y_value, shap_[i], 0, width=0.05, head_length= 0.2*np.abs(shap_[i]), length_includes_head=True, color='#5bc2ae')\n",
    "                if type(original_[i]) == str:\n",
    "                    plt.text(current_value+0.005, y_value, '%s = %s' % (features_[i], original_[i]), horizontalalignment='left', verticalalignment='center', fontsize=10)\n",
    "                else:\n",
    "                    plt.text(current_value+0.005, y_value, '%s (Imputed)' % features_[i], horizontalalignment='left', verticalalignment='center', fontsize=10)\n",
    "            else:\n",
    "                plt.arrow(current_value, y_value, shap_[i], 0, width=0.05, head_length= 0.2*np.abs(shap_[i]), length_includes_head=True, color='#d93f20')\n",
    "                if type(original_[i]) == str:\n",
    "                    plt.text(current_value-0.005, y_value, '%s = %s' % (features_[i], original_[i]), horizontalalignment='right', verticalalignment='center', fontsize=10)\n",
    "                else:\n",
    "                    plt.text(current_value-0.005, y_value, '%s (Imputed)' % features_[i], horizontalalignment='right', verticalalignment='center', fontsize=10)\n",
    "            current_value += shap_[i]\n",
    "            y_value += -0.3\n",
    "        \n",
    "    # end line\n",
    "    plt.plot([current_value,current_value],[y_value+0.15,0.1],'k--',linewidth=1)\n",
    "    if current_value < 0.5:\n",
    "        plt.text(current_value,0.1,'%0.1f%%' % (current_value*100), horizontalalignment='center', verticalalignment='bottom', color='#5bc2ae', fontsize=16, weight='bold')\n",
    "    else:\n",
    "        plt.text(current_value,0.1,'%0.1f%%' % (current_value*100), horizontalalignment='center', verticalalignment='bottom', color='#d93f20', fontsize=16, weight='bold')\n",
    "    plt.text(current_value,0.75,'Posterior', horizontalalignment='center', verticalalignment='bottom', color='black', fontsize=16)\n",
    "        \n",
    "    # limits\n",
    "    plt.xlim(-0.02,1.02)\n",
    "    plt.ylim(y_value-0.05, 0.8)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over patients\n",
    "for a,sample in enumerate(samples):\n",
    "    if y_vector[a]==1:\n",
    "    \n",
    "        # get nonzero shap and feature values\n",
    "        features_ = [feature.split(' # ')[-1] for feature in merged_features if shap_value.at[sample,feature] != 0]\n",
    "        shap_ = shap_value.loc[sample][['gene_all # %s' % x for x in features_]].tolist()\n",
    "        original_ = X_matrix.loc[sample][['gene_all # %s' % x for x in features_]].tolist()\n",
    "\n",
    "        # convert numerical values to strings\n",
    "        for j in range(len(original_)):\n",
    "            if not type(original_[j]) == str:\n",
    "                if type(original_[j]) == bool:\n",
    "                    original_[j] = str(original_[j])\n",
    "                elif not np.isnan(original_[j]):\n",
    "                    if original_[j] == int(original_[j]):\n",
    "                        original_[j] = str(int(original_[j]))\n",
    "                    else:\n",
    "                        original_[j] = str(original_[j])             \n",
    "\n",
    "        # order features based on absolute shap value\n",
    "        sort_index = np.argsort(np.abs(shap_))[::-1]\n",
    "        features_ = [features_[i] for i in sort_index]\n",
    "        shap_ = [shap_[i] for i in sort_index]\n",
    "        original_ = [original_[i] for i in sort_index]\n",
    "\n",
    "        # create figure\n",
    "        plot_patient(sample, y_vector[a], expected_value[a], features_, shap_, original_)\n",
    "        plt.savefig('%s/%s/%s/shap_values/patients/%s.png' % (folder_name, dataset_name, output_folder, sample), bbox_inches='tight', dpi=400)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
