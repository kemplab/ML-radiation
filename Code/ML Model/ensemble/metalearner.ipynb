{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'compareclassifiers_gbm'\n",
    "datasets = [['clinical','gene_all','mutation_onehot_all']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate SHAP values and interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_shap_values = [False]\n",
    "compute_shap_interactions = [False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_trainvalidation_test = 20\n",
    "k_train_validation = 5\n",
    "early_stopping_size = 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgBoost stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hyperopt_iterations = 2**8\n",
    "optimize_learning_rate = True\n",
    "default_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1\n",
    "\n",
    "# implement seed\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_function(parameters):\n",
    "\n",
    "    # load data\n",
    "    with open('_files/data_.pickle', 'rb') as f:\n",
    "        X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # calculate performance\n",
    "    mean_validation_weightedlogloss = hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters)\n",
    "    \n",
    "    # return performance\n",
    "    return {'loss':mean_validation_weightedlogloss, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters):\n",
    "    \n",
    "    # initialize validation performance\n",
    "    validation_weightedlogloss = []\n",
    "    \n",
    "    # iterate over number of training/validation splits\n",
    "    for i in range(k_train_validation):\n",
    "\n",
    "        # xgb datasets\n",
    "        xgb_training = xgb.DMatrix(X_training_train[i], label=y_training_train[i])\n",
    "        xgb_earlystopping = xgb.DMatrix(X_earlystopping_train[i], label=y_earlystopping_train[i])\n",
    "        xgb_validation = xgb.DMatrix(X_validation[i], label=y_validation[i])\n",
    "\n",
    "        # parameters\n",
    "        param = parameters.copy()\n",
    "        param['objective'] = 'multi:softprob'\n",
    "        param['num_class'] = len(datasets[a])\n",
    "        param['eval_metric'] = 'mlogloss'\n",
    "        param['seed'] = seed_\n",
    "        evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "        # train on training\n",
    "        bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "        # evaluate on validation\n",
    "        y_pred = bst.predict(xgb_validation, ntree_limit=bst.best_ntree_limit)\n",
    "        weightedlogloss = log_loss(y_validation[i], y_pred, labels=list(range(len(datasets[a]))))\n",
    "        validation_weightedlogloss.append(weightedlogloss)\n",
    "    \n",
    "    # average validation performance over all folds\n",
    "    mean_validation_weightedlogloss = np.mean(validation_weightedlogloss) + np.std(validation_weightedlogloss)/np.sqrt(len(validation_weightedlogloss))\n",
    "    return mean_validation_weightedlogloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_values(explainer, X_test, tree_limit):\n",
    "    \n",
    "    # compute values\n",
    "    shap_values = explainer.shap_values(X_test, tree_limit=tree_limit)\n",
    "    \n",
    "    # merge features\n",
    "    if len(categorical_conversion) > 0:\n",
    "        shap_values_ = np.zeros((X_test.shape[0], len(merged_features[c])))\n",
    "        for i, feature in enumerate(merged_features[c]):\n",
    "            if feature not in categorical_conversion:\n",
    "                shap_values_[:,i] = shap_values[:,features[c].index(feature)]\n",
    "            else:\n",
    "                find_indices = [j for j,x in enumerate(features[c]) if x.split(' | ')[0] == feature]\n",
    "                shap_values_[:,i] = shap_values[:,find_indices].sum(axis=1)\n",
    "        shap_values = shap_values_.copy()\n",
    "\n",
    "    # return results\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_interactions(explainer, X_test, tree_limit):\n",
    "    \n",
    "    # compute values\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X_test, tree_limit=tree_limit)[0]\n",
    "\n",
    "    # merge features\n",
    "    if len(categorical_conversion) > 0:\n",
    "        shap_interaction_values_ = np.zeros((len(merged_features[c]), len(features[c])))\n",
    "        for i,feature in enumerate(merged_features[c]):\n",
    "            if feature not in categorical_conversion:\n",
    "                shap_interaction_values_[i,:] = shap_interaction_values[features[c].index(feature),:]\n",
    "            else:\n",
    "                find_indices = [j for j,x in enumerate(features[c]) if x.split(' | ')[0] == feature]\n",
    "                shap_interaction_values_[i,:] = shap_interaction_values[find_indices,:].sum(axis=0)\n",
    "        shap_interaction_values = np.zeros((len(merged_features[c]), len(merged_features[c])))\n",
    "        for i,feature in enumerate(merged_features[c]):\n",
    "            if feature not in categorical_conversion:\n",
    "                shap_interaction_values[:,i] = shap_interaction_values_[:,features[c].index(feature)]\n",
    "            else:\n",
    "                find_indices = [j for j,x in enumerate(features[c]) if x.split(' | ')[0] == feature]\n",
    "                shap_interaction_values[:,i] = shap_interaction_values_[:,find_indices].sum(axis=1)\n",
    "\n",
    "    # return results\n",
    "    return shap_interaction_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_y(y):\n",
    "    \n",
    "    dummy_y_ = [[],[]]\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            dummy_y_[0].append(1)\n",
    "            dummy_y_[1].append(0)\n",
    "        else:\n",
    "            dummy_y_[0].append(0)\n",
    "            dummy_y_[1].append(1)\n",
    "    dummy_y_ = np.array(dummy_y_).T\n",
    "    return dummy_y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset names\n",
    "dataset_names = []\n",
    "for a in range(len(datasets)):\n",
    "    dataset_names.append('+'.join(datasets[a]))\n",
    "    os.mkdir('%s/%s' % (output_folder, dataset_names[a]))\n",
    "    \n",
    "# performance files\n",
    "performance_files_weightedlogloss = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_weightedlogloss.to_csv('%s/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "performance_files_balancedaccuracy = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_balancedaccuracy.to_csv('%s/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "performance_files_auroc = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_auroc.to_csv('%s/auroc.csv' % output_folder)\n",
    "\n",
    "performance_files_sensitivity_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_sensitivity_50.to_csv('%s/sensitivity_50.csv' % output_folder)\n",
    "\n",
    "performance_files_specificity_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_specificity_50.to_csv('%s/specificity_50.csv' % output_folder)\n",
    "\n",
    "performance_files_ppv_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_ppv_50.to_csv('%s/ppv_50.csv' % output_folder)\n",
    "\n",
    "performance_files_npv_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_npv_50.to_csv('%s/npv_50.csv' % output_folder)\n",
    "\n",
    "performance_files_optimal_threshold = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_optimal_threshold.to_csv('%s/optimal_threshold.csv' % output_folder)\n",
    "\n",
    "performance_files_sensitivity_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_sensitivity_optimal.to_csv('%s/sensitivity_optimal.csv' % output_folder)\n",
    "\n",
    "performance_files_specificity_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_specificity_optimal.to_csv('%s/specificity_optimal.csv' % output_folder)\n",
    "\n",
    "performance_files_ppv_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_ppv_optimal.to_csv('%s/ppv_optimal.csv' % output_folder)\n",
    "\n",
    "performance_files_npv_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_npv_optimal.to_csv('%s/npv_optimal.csv' % output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "DATASET: clinical+gene_all+mutation_onehot_all\n",
      "-------------------------\n",
      "Split 1\n",
      "clinical: 488/732 - 66.67% - 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 720/732 [00:54<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 179/732 - 24.45% - 37 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [02:46<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 65/732 - 8.88% - 104 features\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [23:51<00:00,  5.17s/it, best loss: 0.700401296672053]\n",
      "Split 2\n",
      "clinical: 530/732 - 72.40% - 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 725/732 [01:14<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 115/732 - 15.71% - 84 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [03:51<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 87/732 - 11.89% - 231 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [15:39<00:00,  1.91s/it, best loss: 0.7061757358009818]\n",
      "Split 3\n",
      "clinical: 467/732 - 63.80% - 45 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [01:13<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 195/732 - 26.64% - 87 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 727/732 [02:25<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 70/732 - 9.56% - 411 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [29:31<00:00,  4.89s/it, best loss: 0.7004384729559482]\n",
      "Split 4\n",
      "clinical: 490/732 - 66.94% - 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 727/732 [02:41<00:01]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 154/732 - 21.04% - 203 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 730/732 [08:23<00:01]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 88/732 - 12.02% - 253 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [38:27<00:00, 10.66s/it, best loss: 0.7266418334007811]\n",
      "Split 5\n",
      "clinical: 515/732 - 70.36% - 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [02:06<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 143/732 - 19.54% - 145 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 730/732 [01:43<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 74/732 - 10.11% - 256 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [31:23<00:00,  4.88s/it, best loss: 0.7036763336041731]\n",
      "Split 6\n",
      "clinical: 485/732 - 66.26% - 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [02:24<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 153/732 - 20.90% - 133 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [01:05<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 94/732 - 12.84% - 70 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [19:48<00:00,  5.30s/it, best loss: 0.7609836441523247]\n",
      "Split 7\n",
      "clinical: 494/732 - 67.49% - 108 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 727/732 [01:16<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 144/732 - 19.67% - 77 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 723/732 [00:57<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 94/732 - 12.84% - 44 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [16:24<00:00,  1.30it/s, best loss: 0.7037753293109309]\n",
      "Split 8\n",
      "clinical: 497/732 - 67.90% - 33 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 722/732 [01:02<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 150/732 - 20.49% - 57 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 727/732 [01:25<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 85/732 - 11.61% - 151 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [09:23<00:00,  1.11s/it, best loss: 0.7202714793280058]\n",
      "Split 9\n",
      "clinical: 531/732 - 72.54% - 67 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 727/732 [02:13<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 104/732 - 14.21% - 150 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 723/732 [01:03<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 97/732 - 13.25% - 32 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:43<00:00,  1.64s/it, best loss: 0.6847444192403176]\n",
      "Split 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|=================== | 687/732 [00:11<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical: 505/732 - 68.99% - 117 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [00:41<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 149/732 - 20.36% - 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [03:22<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 78/732 - 10.66% - 315 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [27:52<00:00,  7.66s/it, best loss: 0.6978496960924832]\n",
      "Split 11\n",
      "clinical: 547/732 - 74.73% - 80 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 728/732 [02:15<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 116/732 - 15.85% - 90 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 730/732 [04:10<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 69/732 - 9.43% - 373 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [38:40<00:00, 14.44s/it, best loss: 0.6465950465835597]\n",
      "Split 12\n",
      "clinical: 523/732 - 71.45% - 54 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 728/732 [01:17<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 134/732 - 18.31% - 73 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [04:15<00:01]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 75/732 - 10.25% - 43 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [19:24<00:00,  1.99s/it, best loss: 0.6979567283116725]\n",
      "Split 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|=================== | 695/732 [00:17<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical: 509/732 - 69.54% - 107 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 728/732 [01:17<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 139/732 - 18.99% - 82 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [01:16<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 84/732 - 11.48% - 113 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [14:35<00:00,  3.54s/it, best loss: 0.6898335687210945]\n",
      "Split 14\n",
      "clinical: 460/732 - 62.84% - 42 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [03:50<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 195/732 - 26.64% - 150 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [03:36<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 77/732 - 10.52% - 68 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [28:00<00:00,  4.08s/it, best loss: 0.7553555908222443]\n",
      "Split 15\n",
      "clinical: 444/732 - 60.66% - 49 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 721/732 [00:42<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 210/732 - 28.69% - 59 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 720/732 [00:59<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 78/732 - 10.66% - 32 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [16:11<00:00,  2.86s/it, best loss: 0.7549177827020466]\n",
      "Split 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 728/732 [00:30<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical: 505/732 - 68.99% - 93 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [01:27<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 150/732 - 20.49% - 70 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [02:17<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 77/732 - 10.52% - 282 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [34:12<00:00,  4.76s/it, best loss: 0.6631834268950008]\n",
      "Split 17\n",
      "clinical: 524/732 - 71.58% - 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [04:04<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 134/732 - 18.31% - 262 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 718/732 [00:40<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 74/732 - 10.11% - 22 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [20:12<00:00,  3.57s/it, best loss: 0.6868844655386104]\n",
      "Split 18\n",
      "clinical: 502/732 - 68.58% - 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [01:19<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 126/732 - 17.21% - 77 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 728/732 [01:03<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 104/732 - 14.21% - 110 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [19:59<00:00,  4.57s/it, best loss: 0.7424548796484389]\n",
      "Split 19\n",
      "clinical: 544/732 - 74.32% - 74 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [03:13<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 114/732 - 15.57% - 101 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 730/732 [01:08<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 74/732 - 10.11% - 142 features\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [48:40<00:00, 15.15s/it, best loss: 0.615193971648878]\n",
      "Split 20\n",
      "clinical: 523/732 - 71.45% - 88 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 730/732 [02:42<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 125/732 - 17.08% - 179 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [02:42<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 84/732 - 11.48% - 83 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [27:03<00:00,  9.20s/it, best loss: 0.6530182038523007]\n"
     ]
    }
   ],
   "source": [
    "# iterate over datasets\n",
    "for a in range(len(datasets)):\n",
    "    print('-------------------------')\n",
    "    print('DATASET: %s' % dataset_names[a])\n",
    "    print('-------------------------')\n",
    "    \n",
    "    # iterate over number of training+validation/testing splits\n",
    "    for b in range(n_splits_trainvalidation_test):\n",
    "    \n",
    "        print('Split %d' % (b+1))\n",
    "        \n",
    "        # load categorical conversion from each dataset\n",
    "        features = []\n",
    "        merged_features = []\n",
    "        with open('_datasets/%s.pickle' % datasets[a][0], 'rb') as f:\n",
    "            X_matrix, y_vector, categorical_conversion_old = pickle.load(f, encoding='latin1')\n",
    "        features.append(['%s # %s' % (datasets[a][0], x) for x in X_matrix.columns.tolist()])\n",
    "        categorical_conversion = {}\n",
    "        for key in categorical_conversion_old:\n",
    "            categorical_conversion['%s # %s' % (datasets[a][0], key)] = categorical_conversion_old[key]\n",
    "        if len(categorical_conversion) > 0:\n",
    "            merged_features.append([])\n",
    "            for feature in features[0]:\n",
    "                if feature.split(' | ')[0] not in categorical_conversion:\n",
    "                    merged_features[-1].append(feature)\n",
    "                elif feature.split(' | ')[0] not in merged_features[-1]:\n",
    "                    merged_features[-1].append(feature.split(' | ')[0])\n",
    "        else:\n",
    "            merged_features.append(features[0].copy())\n",
    "        for c in range(1,len(datasets[a])):\n",
    "            with open('_datasets/%s.pickle' % datasets[a][c], 'rb') as f:\n",
    "                X_matrix_, y_vector_, categorical_conversion_old = pickle.load(f, encoding='latin1')\n",
    "            features.append(['%s # %s' % (datasets[a][c], x) for x in X_matrix_.columns.tolist()])\n",
    "            categorical_conversion_ = {}\n",
    "            for key in categorical_conversion_old:\n",
    "                categorical_conversion_['%s # %s' % (datasets[a][c], key)] = categorical_conversion_old[key]\n",
    "            categorical_conversion = {**categorical_conversion, **categorical_conversion_}\n",
    "            if len(categorical_conversion_) > 0:\n",
    "                merged_features.append([])\n",
    "                for feature in features[c]:\n",
    "                    if feature.split(' | ')[0] not in categorical_conversion_:\n",
    "                        merged_features[-1].append(feature)\n",
    "                    elif feature.split(' | ')[0] not in merged_features[-1]:\n",
    "                        merged_features[-1].append(feature.split(' | ')[0])\n",
    "            else:\n",
    "                merged_features.append(features[c].copy())\n",
    "        \n",
    "        # load results from individual datasets\n",
    "        validation_X = []\n",
    "        validation_predictions = []\n",
    "        X_test = []\n",
    "        y_pred = []\n",
    "        explainers_independent = []\n",
    "        explainers_dependent = []\n",
    "        tree_limit = []\n",
    "        for c in range(len(datasets[a])):\n",
    "            with open('%s/_individual/%s/iter_%d.pickle' % (output_folder, datasets[a][c], b+1), 'rb') as f:\n",
    "                validation_X_, validation_y, validation_predictions_, X_test_, y_test, y_pred_, explainer_independent, explainer_dependent, tree_limit_ = pickle.load(f)\n",
    "            validation_X.append(validation_X_)\n",
    "            validation_predictions.append(validation_predictions_)\n",
    "            X_test.append(X_test_)\n",
    "            y_pred.append(y_pred_)\n",
    "            explainers_independent.append(explainer_independent)\n",
    "            explainers_dependent.append(explainer_dependent)\n",
    "            tree_limit.append(tree_limit_)\n",
    "\n",
    "        # combine predictions\n",
    "        validation_predictions = np.concatenate([x.reshape(-1,1) for x in validation_predictions], axis=1)\n",
    "        test_predictions = np.concatenate([x.reshape(-1,1) for x in y_pred], axis=1)\n",
    "        \n",
    "        # if more than one dataset\n",
    "        if len(datasets[a]) > 1:\n",
    "        \n",
    "            # validation best classifier\n",
    "            validation_best_classifier = []\n",
    "            for i in range(len(validation_y)):\n",
    "                if validation_y[i] == 0:\n",
    "                    validation_best_classifier.append(np.argmin(validation_predictions[i,:]))\n",
    "                elif validation_y[i] == 1:\n",
    "                    validation_best_classifier.append(np.argmax(validation_predictions[i,:]))\n",
    "            validation_best_classifier = np.array(validation_best_classifier)\n",
    "\n",
    "            # subset features that are in any of the models\n",
    "            features_in_models = []\n",
    "            for c in range(len(explainers_independent)):\n",
    "                shapval = explainers_independent[c].shap_values(validation_X[c], tree_limit=tree_limit[c]).mean(axis=0)\n",
    "                features_in_models.extend([features[c][i] for i in range(len(features[c])) if shapval[i] != 0])\n",
    "                print('%s: %d/%d - %0.2f%% - %d features' % (datasets[a][c], len([x for x in validation_best_classifier if x==c]), len(validation_best_classifier), len([x for x in validation_best_classifier if x==c])/len(validation_best_classifier)*100, len([features[c][i] for i in range(len(features[c])) if shapval[i] != 0])))\n",
    "\n",
    "            # get combined dataset with features in models\n",
    "            validation_X_all = pd.concat(validation_X, axis=1)[features_in_models]\n",
    "            X_test_all = pd.concat(X_test, axis=1)[features_in_models]\n",
    "\n",
    "            # separate training_full from earlystopping_full\n",
    "            training_index = []\n",
    "            earlystopping_index = []\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "            for training_, earlystopping_ in sss.split(validation_X_all, validation_best_classifier):\n",
    "                training_index.append(list(training_))\n",
    "                earlystopping_index.append(list(earlystopping_))\n",
    "            training_full_X = validation_X_all.iloc[training_index[0],]\n",
    "            earlystopping_full_X = validation_X_all.iloc[earlystopping_index[0],]\n",
    "            training_full_y = validation_best_classifier[training_index[0]]\n",
    "            earlystopping_full_y = validation_best_classifier[earlystopping_index[0]]\n",
    "\n",
    "            # separate full\n",
    "            sep1_index = []\n",
    "            sep2_index = []\n",
    "            skf = StratifiedKFold(n_splits=k_train_validation, shuffle=True, random_state=seed_)\n",
    "            for sep1_, sep2_ in skf.split(validation_X_all, validation_best_classifier):\n",
    "                sep1_index.append(list(sep1_))\n",
    "                sep2_index.append(list(sep2_))\n",
    "            sep1_X = []\n",
    "            sep2_X = []\n",
    "            sep1_y = []\n",
    "            sep2_y = []\n",
    "            for c in range(k_train_validation):\n",
    "                sep1_X.append(validation_X_all.iloc[sep1_index[c],])\n",
    "                sep2_X.append(validation_X_all.iloc[sep2_index[c],])\n",
    "                sep1_y.append(validation_best_classifier[sep1_index[c]])\n",
    "                sep2_y.append(validation_best_classifier[sep2_index[c]])\n",
    "\n",
    "            # separate training_sep1 from earlystopping_sep1\n",
    "            training_sep1_X = []\n",
    "            earlystopping_sep1_X = []\n",
    "            training_sep1_y = []\n",
    "            earlystopping_sep1_y = []\n",
    "            for c in range(k_train_validation):\n",
    "                training_index = []\n",
    "                earlystopping_index = []\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "                for training_, earlystopping_ in sss.split(sep1_X[c], sep1_y[c]):\n",
    "                    training_index.append(list(training_))\n",
    "                    earlystopping_index.append(list(earlystopping_))\n",
    "                training_sep1_X.append(sep1_X[c].iloc[training_index[0],])\n",
    "                earlystopping_sep1_X.append(sep1_X[c].iloc[earlystopping_index[0],])\n",
    "                training_sep1_y.append(sep1_y[c][training_index[0]])\n",
    "                earlystopping_sep1_y.append(sep1_y[c][earlystopping_index[0]])\n",
    "\n",
    "            # xgb parameter values\n",
    "            parameters = {\n",
    "                'gamma': hp.loguniform('gamma', np.log(0.0001), np.log(5)) - 0.0001,\n",
    "                'max_depth': scope.int(hp.uniform('max_depth', 1, 11)),\n",
    "                'min_child_weight': scope.int(hp.loguniform('min_child_weight', np.log(1), np.log(100))),\n",
    "                'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "                'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "                'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n",
    "                'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(4)),\n",
    "                'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)) - 0.0001\n",
    "                         }\n",
    "            # xgb learning rate\n",
    "            if optimize_learning_rate:\n",
    "                parameters['eta'] = hp.loguniform('eta', np.log(0.01), np.log(0.5))\n",
    "            else:\n",
    "                parameters['eta'] = hp.choice('eta', [default_learning_rate])\n",
    "\n",
    "            # save info for hyperopt\n",
    "            with open('_files/data_.pickle','wb') as f:\n",
    "                pickle.dump([training_sep1_X, training_sep1_y, earlystopping_sep1_X, earlystopping_sep1_y, sep2_X, sep2_y], f)\n",
    "\n",
    "            # hyperopt to find best parameters\n",
    "            trials = Trials()\n",
    "            best = fmin(hyperopt_function, parameters, algo=tpe.suggest, max_evals=n_hyperopt_iterations, trials=trials, rstate=np.random.RandomState(seed_), verbose=0, show_progressbar=True)\n",
    "\n",
    "            # create classifier using best parameters\n",
    "            xgb_training = xgb.DMatrix(training_full_X, label=training_full_y)\n",
    "            xgb_earlystopping = xgb.DMatrix(earlystopping_full_X, label=earlystopping_full_y)\n",
    "            xgb_test = xgb.DMatrix(X_test_all)\n",
    "\n",
    "            if optimize_learning_rate:\n",
    "                param = {'objective':'multi:softprob', 'num_class':len(datasets[a]), 'eval_metric':'mlogloss', 'seed':seed_, 'eta':best['eta'], 'gamma':best['gamma'], 'max_depth':int(best['max_depth']), 'min_child_weight':int(best['min_child_weight']), 'subsample':best['subsample'], 'colsample_bytree':best['colsample_bytree'], 'colsample_bylevel':best['colsample_bylevel'], 'reg_lambda':best['reg_lambda'], 'reg_alpha':best['reg_alpha']}\n",
    "            else:\n",
    "                param = {'objective':'multi:softprob', 'num_class':len(datasets[a]), 'eval_metric':'mlogloss', 'seed':seed_, 'eta':default_learning_rate, 'gamma':best['gamma'], 'max_depth':int(best['max_depth']), 'min_child_weight':int(best['min_child_weight']), 'subsample':best['subsample'], 'colsample_bytree':best['colsample_bytree'], 'colsample_bylevel':best['colsample_bylevel'], 'reg_lambda':best['reg_lambda'], 'reg_alpha':best['reg_alpha']}\n",
    "            evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "            # train stacker\n",
    "            bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "            # get weights on testing set\n",
    "            weights = bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit)\n",
    "            \n",
    "            # calculate stacker performance - log loss\n",
    "            test_best_classifier = []\n",
    "            for i in range(len(y_test)):\n",
    "                if y_test[i] == 0:\n",
    "                    test_best_classifier.append(np.argmin(test_predictions[i,:]))\n",
    "                elif y_test[i] == 1:\n",
    "                    test_best_classifier.append(np.argmax(test_predictions[i,:]))\n",
    "            test_best_classifier = np.array(test_best_classifier)\n",
    "            \n",
    "            # save stacker predictions\n",
    "            with open('%s/%s/stacker_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "                pickle.dump([X_test, X_test_all, y_test, y_pred, test_predictions, weights, test_best_classifier, bst], f)\n",
    "            \n",
    "            # get predictions on test set\n",
    "            y_pred = []\n",
    "            for i in range(len(y_test)):\n",
    "                y_pred.append(np.average(test_predictions[i,:], weights=weights[i,:]))\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "        # if only one dataset\n",
    "        else:\n",
    "            y_pred = y_pred[0]\n",
    "            weights = np.array([1 for x in y_test]).reshape(-1,1)\n",
    "        \n",
    "        # save predictions\n",
    "        with open('%s/%s/predictions_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "            pickle.dump([X_test[0].index.tolist(), y_test, y_pred], f)\n",
    "        \n",
    "        # calculate test performance - weighted log loss\n",
    "        pos_weight = len([x for x in y_test if x==0])/len([x for x in y_test if x==1])\n",
    "        sample_weights = [pos_weight if x==1 else 1 for x in y_test]\n",
    "        performance = log_loss(y_test, y_pred, sample_weight=sample_weights)\n",
    "        performance_files_weightedlogloss.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_weightedlogloss.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_weightedlogloss.at['STERR', dataset_names[a]] = np.nanstd(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_weightedlogloss.to_csv('%s/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - balanced accuracy\n",
    "        y_pred_ = [1 if x>=0.5 else 0 for x in y_pred]\n",
    "        performance = balanced_accuracy_score(y_test, y_pred_)\n",
    "        performance_files_balancedaccuracy.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_balancedaccuracy.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_balancedaccuracy.at['STERR', dataset_names[a]] = np.nanstd(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_balancedaccuracy.to_csv('%s/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - auroc\n",
    "        y_pred_ = np.concatenate((np.array([1-x for x in y_pred]).reshape(-1,1), y_pred.reshape(-1,1)), axis=1)\n",
    "        performance = roc_auc_score(dummy_y(y_test), y_pred_)\n",
    "        performance_files_auroc.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_auroc.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_auroc.at['STERR', dataset_names[a]] = np.nanstd(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_auroc.to_csv('%s/auroc.csv' % output_folder)\n",
    "        \n",
    "        # tp, tn, fp, fn\n",
    "        tp = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]>0.5])\n",
    "        fp = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]>0.5])\n",
    "        tn = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]<0.5])\n",
    "        fn = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]<0.5])\n",
    "        \n",
    "        # calculate test performance - sensitivity - 50\n",
    "        performance = tp/(tp+fn)\n",
    "        performance_files_sensitivity_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_sensitivity_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_sensitivity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_sensitivity_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_sensitivity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_sensitivity_50.to_csv('%s/sensitivity_50.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - specificity - 50\n",
    "        performance = tn/(tn+fp)\n",
    "        performance_files_specificity_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_specificity_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_specificity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_specificity_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_specificity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_specificity_50.to_csv('%s/specificity_50.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - ppv - 50\n",
    "        performance = tp/(tp+fp)\n",
    "        performance_files_ppv_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_ppv_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_ppv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_ppv_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_ppv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_ppv_50.to_csv('%s/ppv_50.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - npv - 50\n",
    "        performance = tn/(tn+fn)\n",
    "        performance_files_npv_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_npv_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_npv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_npv_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_npv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_npv_50.to_csv('%s/npv_50.csv' % output_folder)\n",
    "        \n",
    "        # optimal threshold\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        youden = [(1-fpr[i])+tpr[i] for i in range(len(thresholds))]\n",
    "        top_index = []\n",
    "        top_threshold = []\n",
    "        for i in range(len(youden)):\n",
    "            if youden[i] == np.max(youden):\n",
    "                top_index.append(i)\n",
    "                top_threshold.append(thresholds[i])\n",
    "        distance_from_50 = [np.abs(x-0.5) for x in top_threshold]\n",
    "        top_index = top_index[np.argmin(distance_from_50)]\n",
    "        optimal_threshold = top_threshold[np.argmin(distance_from_50)]\n",
    "        performance_files_optimal_threshold.at['split_%d' % (b+1), dataset_names[a]] = optimal_threshold\n",
    "        performance_files_optimal_threshold.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_optimal_threshold.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_optimal_threshold.at['STERR', dataset_names[a]] = np.nanstd(performance_files_optimal_threshold.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_optimal_threshold.to_csv('%s/optimal_threshold.csv' % output_folder)\n",
    "        \n",
    "        # tp, tn, fp, fn\n",
    "        tp = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]>optimal_threshold])\n",
    "        fp = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]>optimal_threshold])\n",
    "        tn = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]<optimal_threshold])\n",
    "        fn = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]<optimal_threshold])\n",
    "        \n",
    "        # calculate test performance - sensitivity - optimal\n",
    "        performance = tp/(tp+fn)\n",
    "        performance_files_sensitivity_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_sensitivity_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_sensitivity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_sensitivity_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_sensitivity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_sensitivity_optimal.to_csv('%s/sensitivity_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - specificity - optimal\n",
    "        performance = tn/(tn+fp)\n",
    "        performance_files_specificity_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_specificity_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_specificity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_specificity_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_specificity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_specificity_optimal.to_csv('%s/specificity_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - ppv - optimal\n",
    "        performance = tp/(tp+fp)\n",
    "        performance_files_ppv_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_ppv_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_ppv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_ppv_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_ppv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_ppv_optimal.to_csv('%s/ppv_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - npv - optimal\n",
    "        performance = tn/(tn+fn)\n",
    "        performance_files_npv_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_npv_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_npv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_npv_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_npv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_npv_optimal.to_csv('%s/npv_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate shap values\n",
    "        if compute_shap_values[a]:\n",
    "        \n",
    "            # initialize shap values\n",
    "            shap_values = []\n",
    "\n",
    "            # iterate over datasets\n",
    "            for c in range(len(datasets[a])):\n",
    "\n",
    "                # compute shap values\n",
    "                shapval = calculate_shap_values(explainers_independent[c], X_test[c], tree_limit[c])\n",
    "\n",
    "                # weight classifier shap values for each sample\n",
    "                for i in range(len(y_test)):\n",
    "                    shapval[i,:] *= weights[i,c]\n",
    "                shap_values.append(pd.DataFrame(data=shapval, index=X_test[c].index.tolist(), columns=merged_features[c]))\n",
    "\n",
    "            # merge multi-model shap values\n",
    "            if len(shap_values) > 1:\n",
    "                shap_values = pd.concat(shap_values, axis=1, sort=False)\n",
    "            else:\n",
    "                shap_values = shap_values[0]\n",
    "\n",
    "            # expected values\n",
    "            expected = []\n",
    "            for c in range(len(datasets[a])):\n",
    "                expected.append(explainers_independent[c].expected_value)\n",
    "            expected = np.array(expected)\n",
    "\n",
    "            # individual expected values\n",
    "            shap_expected = []\n",
    "            for i in range(len(y_test)):\n",
    "                shap_expected.append(np.average(expected, weights=weights[i,:]))\n",
    "\n",
    "            # save results\n",
    "            with open('%s/%s/shap_values_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "                pickle.dump([shap_expected, shap_values], f)\n",
    "                \n",
    "        # calculate shap interactions\n",
    "        if compute_shap_interactions[a]:\n",
    "                                                                                                      \n",
    "            # iterate over samples\n",
    "            for i in tqdm(range(len(y_test))):\n",
    "                                                                                                                                         \n",
    "                # initialize sample results\n",
    "                sample_shap = []\n",
    "                \n",
    "                # iterate over datasets\n",
    "                for c in range(len(datasets[a])):\n",
    "                        \n",
    "                    # calculate shap interaction values\n",
    "                    sample_shap.append(pd.DataFrame(data=weights[i,c] * calculate_shap_interactions(explainers_dependent[c], X_test[c].iloc[i:i+1,:], tree_limit[c]), index=merged_features[c], columns=merged_features[c]))\n",
    "                        \n",
    "                # merge multi-class shap values\n",
    "                if len(sample_shap) > 1:\n",
    "                    sample_shap_ = pd.concat(sample_shap, axis=1, sort=False).fillna(0)\n",
    "                else:\n",
    "                    sample_shap_ = sample_shap[0]\n",
    "                \n",
    "                # add to overall array\n",
    "                if i==0:\n",
    "                    shap_interaction_values = sample_shap_.copy()\n",
    "                else:\n",
    "                    shap_interaction_values += sample_shap_\n",
    "                \n",
    "            # divide by total number of samples\n",
    "            shap_interaction_values /= len(y_test)\n",
    "            \n",
    "            # save results\n",
    "            with open('%s/%s/shap_interactions_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "                pickle.dump(shap_interaction_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
