{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'compareclassifiers_nohyperopt'\n",
    "\n",
    "if os.path.isdir(output_folder):\n",
    "    raise Exception('Already run!')\n",
    "else:\n",
    "    os.mkdir(output_folder)\n",
    "    os.mkdir('%s/_individual' % output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['clinical','gene_all','mutation_onehot_all']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hyperopt_iterations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_trainvalidation_test = 20\n",
    "test_size = 0.2\n",
    "k_train_validation = 5\n",
    "early_stopping_size = 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1\n",
    "\n",
    "# implement seed\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "\n",
    "# timestamp\n",
    "timestamp = 1912111334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_function(parameters):\n",
    "\n",
    "    # load data\n",
    "    with open('_files/data_%s.pickle' % timestamp, 'rb') as f:\n",
    "        X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # calculate performance\n",
    "    mean_validation_weightedlogloss, validation_pred = hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters)\n",
    "    gc.collect()\n",
    "    \n",
    "    # save validation predictions if best classifier\n",
    "    with open('_files/validation_%s.pickle' % timestamp,'rb') as f:\n",
    "        best_weightedlogloss = pickle.load(f)\n",
    "    if mean_validation_weightedlogloss < best_weightedlogloss:\n",
    "        with open('_files/validation_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump(mean_validation_weightedlogloss, f)\n",
    "        with open('_files/validation_xgb_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump(validation_pred, f)\n",
    "    \n",
    "    # return performance\n",
    "    return {'loss':mean_validation_weightedlogloss, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters):\n",
    "    \n",
    "    # initialize validation performance and predictions\n",
    "    validation_weightedlogloss = []\n",
    "    validation_pred = []\n",
    "    \n",
    "    # iterate over number of training/validation splits\n",
    "    for i in range(k_train_validation):\n",
    "        \n",
    "        # positive weight\n",
    "        pos_weight = len([x for x in y_training_train[i] if x==0])/len([x for x in y_training_train[i] if x==1])\n",
    "\n",
    "        # xgb datasets\n",
    "        xgb_training = xgb.DMatrix(X_training_train[i], label=y_training_train[i])\n",
    "        xgb_earlystopping = xgb.DMatrix(X_earlystopping_train[i], label=y_earlystopping_train[i])\n",
    "        xgb_validation = xgb.DMatrix(X_validation[i], label=y_validation[i])\n",
    "\n",
    "        # parameters\n",
    "        param = {}\n",
    "        param['objective'] = 'binary:logistic'\n",
    "        param['eval_metric'] = 'logloss'\n",
    "        param['scale_pos_weight'] = pos_weight\n",
    "        param['seed'] = seed_\n",
    "        evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "        # train on training\n",
    "        bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "        # evaluate on validation\n",
    "        y_pred = bst.predict(xgb_validation, ntree_limit=bst.best_ntree_limit)\n",
    "        pos_weight = len([x for x in y_validation[i] if x==0])/len([x for x in y_validation[i] if x==1])\n",
    "        sample_weights = [pos_weight if x==1 else 1 for x in y_validation[i]]\n",
    "        weightedlogloss = log_loss(y_validation[i], y_pred, sample_weight=sample_weights)\n",
    "        validation_weightedlogloss.append(weightedlogloss)\n",
    "        validation_pred.append(y_pred)\n",
    "    \n",
    "    # average validation performance over all folds\n",
    "    mean_validation_weightedlogloss = np.mean(validation_weightedlogloss) + np.std(validation_weightedlogloss)/np.sqrt(len(validation_weightedlogloss))\n",
    "    return mean_validation_weightedlogloss, np.concatenate(validation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_y(y):\n",
    "    \n",
    "    dummy_y_ = [[],[]]\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            dummy_y_[0].append(1)\n",
    "            dummy_y_[1].append(0)\n",
    "        else:\n",
    "            dummy_y_[0].append(0)\n",
    "            dummy_y_[1].append(1)\n",
    "    dummy_y_ = np.array(dummy_y_).T\n",
    "    return dummy_y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over datasets\n",
    "for a in range(len(datasets)):\n",
    "    \n",
    "    # folder\n",
    "    os.mkdir('%s/_individual/%s' % (output_folder, datasets[a]))\n",
    "    \n",
    "# performance files\n",
    "performance_files_weightedlogloss = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=datasets)\n",
    "performance_files_weightedlogloss.to_csv('%s/_individual/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "performance_files_balancedaccuracy = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=datasets)\n",
    "performance_files_balancedaccuracy.to_csv('%s/_individual/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "performance_files_auroc = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=datasets)\n",
    "performance_files_auroc.to_csv('%s/_individual/auroc.csv' % output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "DATASET: clinical\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.80s/it, best loss: 0.5234342948967727]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.96s/it, best loss: 0.5498212284065885]\n",
      "Split 3\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.93s/it, best loss: 0.45731651967533854]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.12s/it, best loss: 0.5526706851360632]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.23it/s, best loss: 0.5224020361168233]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.67it/s, best loss: 0.5201531971009773]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.22s/it, best loss: 0.5166926986872504]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.82s/it, best loss: 0.5522352220508118]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.13s/it, best loss: 0.5558950547623484]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.92s/it, best loss: 0.4979514733643062]\n",
      "Split 11\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.23s/it, best loss: 0.48277239708922093]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.42s/it, best loss: 0.5974381851267209]\n",
      "Split 13\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it, best loss: 0.528183719378166]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.18s/it, best loss: 0.4970533918812468]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.35s/it, best loss: 0.5898157360690959]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.99s/it, best loss: 0.5372450670435586]\n",
      "Split 17\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.76s/it, best loss: 0.48546609259822915]\n",
      "Split 18\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.50s/it, best loss: 0.46892289376147167]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.68s/it, best loss: 0.5109820181364735]\n",
      "Split 20\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.87s/it, best loss: 0.582294584247291]\n",
      "-------------------------\n",
      "DATASET: gene_all\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:42<00:00, 42.41s/it, best loss: 0.6391058213543551]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:48<00:00, 48.14s/it, best loss: 0.8171998650437858]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:44<00:00, 44.65s/it, best loss: 0.7788179711888351]\n",
      "Split 4\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:45<00:00, 45.07s/it, best loss: 0.751953720068315]\n",
      "Split 5\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:46<00:00, 46.24s/it, best loss: 0.755297749798613]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:40<00:00, 40.62s/it, best loss: 0.7247590386016135]\n",
      "Split 7\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:50<00:00, 50.55s/it, best loss: 0.768389347193621]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:45<00:00, 45.24s/it, best loss: 0.9092580803171846]\n",
      "Split 9\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:47<00:00, 47.57s/it, best loss: 0.787149327319846]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:41<00:00, 41.87s/it, best loss: 0.6924107742945613]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:41<00:00, 41.15s/it, best loss: 0.6886593025226708]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:43<00:00, 43.76s/it, best loss: 0.7092224840543062]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:37<00:00, 37.76s/it, best loss: 0.7620743884971614]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:42<00:00, 42.51s/it, best loss: 0.7532616306745037]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:46<00:00, 46.22s/it, best loss: 0.7747186429398446]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:42<00:00, 42.63s/it, best loss: 0.7107572520904453]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:46<00:00, 46.39s/it, best loss: 0.8166562607817451]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:43<00:00, 43.03s/it, best loss: 0.6549293234318639]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:44<00:00, 44.37s/it, best loss: 0.7411238073002002]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:43<00:00, 43.64s/it, best loss: 0.8172765934459092]\n",
      "-------------------------\n",
      "DATASET: mutation_onehot_all\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:52<00:00, 52.04s/it, best loss: 0.6884627807989813]\n",
      "Split 2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:56<00:00, 56.06s/it, best loss: 0.6301887106906825]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:52<00:00, 52.30s/it, best loss: 0.6696778778461282]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:07<00:00, 67.48s/it, best loss: 0.6943932825601051]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:57<00:00, 57.86s/it, best loss: 0.7155353315359139]\n",
      "Split 6\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:55<00:00, 55.82s/it, best loss: 0.669657269292255]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:02<00:00, 62.17s/it, best loss: 0.6769167978863869]\n",
      "Split 8\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:52<00:00, 52.35s/it, best loss: 0.662097575932681]\n",
      "Split 9\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:55<00:00, 55.02s/it, best loss: 0.6310656969024605]\n",
      "Split 10\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:00<00:00, 60.67s/it, best loss: 0.6771978546689166]\n",
      "Split 11\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:49<00:00, 49.90s/it, best loss: 0.6813841629179884]\n",
      "Split 12\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:01<00:00, 61.64s/it, best loss: 0.7126491163829555]\n",
      "Split 13\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:41<00:00, 41.90s/it, best loss: 0.6400142628937385]\n",
      "Split 14\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:11<00:00, 71.11s/it, best loss: 0.7150682385284447]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:14<00:00, 74.33s/it, best loss: 0.7435645301578754]\n",
      "Split 16\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:13<00:00, 73.47s/it, best loss: 0.7369612014249206]\n",
      "Split 17\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:50<00:00, 50.57s/it, best loss: 0.6476610741302584]\n",
      "Split 18\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:47<00:00, 47.75s/it, best loss: 0.6454544918400695]\n",
      "Split 19\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:06<00:00, 66.38s/it, best loss: 0.6717001632714708]\n",
      "Split 20\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:04<00:00, 64.97s/it, best loss: 0.7210691623540101]\n"
     ]
    }
   ],
   "source": [
    "# iterate over datasets\n",
    "for a in range(len(datasets)):\n",
    "    print('-------------------------')\n",
    "    print('DATASET: %s' % datasets[a])\n",
    "    print('-------------------------')\n",
    "    \n",
    "    # load dataset\n",
    "    with open('_datasets/%s.pickle' % datasets[a], 'rb') as f:\n",
    "        X_matrix, y_vector, categorical_conversion = pickle.load(f, encoding='latin1')\n",
    "    X_matrix.columns = ['%s # %s' % (datasets[a], feature) for feature in X_matrix.columns.tolist()]\n",
    "\n",
    "    # divide train+validation from testing\n",
    "    trainvalidation_index = []\n",
    "    test_index = []\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits_trainvalidation_test, test_size=test_size, random_state=seed_)\n",
    "    for trainvalidation_, test_ in sss.split(X_matrix, y_vector):\n",
    "        trainvalidation_index.append(list(trainvalidation_))\n",
    "        test_index.append(list(test_))\n",
    "\n",
    "    # iterate over number of training+validation/testing splits\n",
    "    for b in range(n_splits_trainvalidation_test):\n",
    "        print('Split %d' % (b+1))\n",
    "        \n",
    "        # separate train+validation and testing\n",
    "        X_trainvalidation = X_matrix.iloc[trainvalidation_index[b],]\n",
    "        X_test = X_matrix.iloc[test_index[b],]\n",
    "        y_trainvalidation = y_vector[trainvalidation_index[b]]\n",
    "        y_test = y_vector[test_index[b]]\n",
    "\n",
    "        # separate training_trainvalidation from earlystopping_trainvalidation\n",
    "        training_index = []\n",
    "        earlystopping_index = []\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "        for training_, earlystopping_ in sss.split(X_trainvalidation, y_trainvalidation):\n",
    "            training_index.append(list(training_))\n",
    "            earlystopping_index.append(list(earlystopping_))\n",
    "        X_training_trainvalidation = X_trainvalidation.iloc[training_index[0],]\n",
    "        X_earlystopping_trainvalidation = X_trainvalidation.iloc[earlystopping_index[0],]\n",
    "        y_training_trainvalidation = y_trainvalidation[training_index[0]]\n",
    "        y_earlystopping_trainvalidation = y_trainvalidation[earlystopping_index[0]]\n",
    "        \n",
    "        # divide train from validation\n",
    "        train_index = []\n",
    "        validation_index = []\n",
    "        skf = StratifiedKFold(n_splits=k_train_validation, shuffle=True, random_state=seed_)\n",
    "        for train_, validation_ in skf.split(X_trainvalidation, y_trainvalidation):\n",
    "            train_index.append(list(train_))\n",
    "            validation_index.append(list(validation_))\n",
    "\n",
    "        # separate train and validation\n",
    "        X_train = []\n",
    "        X_validation = []\n",
    "        y_train = []\n",
    "        y_validation = []\n",
    "        for c in range(k_train_validation):\n",
    "            X_train.append(X_trainvalidation.iloc[train_index[c],])\n",
    "            X_validation.append(X_trainvalidation.iloc[validation_index[c],])\n",
    "            y_train.append(y_trainvalidation[train_index[c]])\n",
    "            y_validation.append(y_trainvalidation[validation_index[c]])\n",
    "        \n",
    "        # separate training_train from earlystopping_train\n",
    "        X_training_train = []\n",
    "        X_earlystopping_train = []\n",
    "        y_training_train = []\n",
    "        y_earlystopping_train = []\n",
    "        for c in range(k_train_validation):\n",
    "            training_index = []\n",
    "            earlystopping_index = []\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "            for training_, earlystopping_ in sss.split(X_train[c], y_train[c]):\n",
    "                training_index.append(list(training_))\n",
    "                earlystopping_index.append(list(earlystopping_))\n",
    "            X_training_train.append(X_train[c].iloc[training_index[0],])\n",
    "            X_earlystopping_train.append(X_train[c].iloc[earlystopping_index[0],])\n",
    "            y_training_train.append(y_train[c][training_index[0]])\n",
    "            y_earlystopping_train.append(y_train[c][earlystopping_index[0]])\n",
    "            \n",
    "        # initialize test predictions\n",
    "        classifier_test_predictions = []\n",
    "        \n",
    "        # initialize shap explainers\n",
    "        explainers = []\n",
    "        \n",
    "        # xgb parameters\n",
    "        parameters = {\n",
    "            'dummy': hp.uniform('dummy', 0, 1),\n",
    "                     }\n",
    "\n",
    "        # save info for hyperopt\n",
    "        with open('_files/validation_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump(1000., f)\n",
    "        with open('_files/data_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump([X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation], f)\n",
    "\n",
    "        # hyperopt to find best parameters\n",
    "        trials = Trials()\n",
    "        best = fmin(hyperopt_function, parameters, algo=tpe.suggest, max_evals=n_hyperopt_iterations, trials=trials, rstate=np.random.RandomState(seed_), verbose=0, show_progressbar=True)\n",
    "            \n",
    "        # create classifier using best parameters\n",
    "        pos_weight = len([x for x in y_training_trainvalidation if x==0])/len([x for x in y_training_trainvalidation if x==1])\n",
    "            \n",
    "        # xgb datasets\n",
    "        xgb_training = xgb.DMatrix(X_training_trainvalidation, label=y_training_trainvalidation)\n",
    "        xgb_earlystopping = xgb.DMatrix(X_earlystopping_trainvalidation, label=y_earlystopping_trainvalidation)\n",
    "        xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # parameters\n",
    "        param = {'objective':'binary:logistic', 'eval_metric':'logloss', 'scale_pos_weight':pos_weight, 'seed':seed_}\n",
    "        evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "        # train on training+validation\n",
    "        bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "                \n",
    "        # predicted probabilities on test set\n",
    "        y_pred = bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "        # calculate test performance - weighted log loss\n",
    "        pos_weight = len([x for x in y_test if x==0])/len([x for x in y_test if x==1])\n",
    "        sample_weights = [pos_weight if x==1 else 1 for x in y_test]\n",
    "        performance = log_loss(y_test, y_pred, sample_weight=sample_weights)\n",
    "        performance_files_weightedlogloss.at['split_%d' % (b+1), datasets[a]] = performance\n",
    "        performance_files_weightedlogloss.at['MEAN', datasets[a]] = np.nanmean(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())\n",
    "        performance_files_weightedlogloss.at['STERR', datasets[a]] = np.nanstd(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_weightedlogloss.to_csv('%s/_individual/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - balanced accuracy\n",
    "        y_pred_ = [1 if x>=0.5 else 0 for x in y_pred]\n",
    "        performance = balanced_accuracy_score(y_test, y_pred_)\n",
    "        performance_files_balancedaccuracy.at['split_%d' % (b+1), datasets[a]] = performance\n",
    "        performance_files_balancedaccuracy.at['MEAN', datasets[a]] = np.nanmean(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())\n",
    "        performance_files_balancedaccuracy.at['STERR', datasets[a]] = np.nanstd(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_balancedaccuracy.to_csv('%s/_individual/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - auroc\n",
    "        y_pred_ = np.concatenate((np.array([1-x for x in y_pred]).reshape(-1,1), y_pred.reshape(-1,1)), axis=1)\n",
    "        performance = roc_auc_score(dummy_y(y_test), y_pred_)\n",
    "        performance_files_auroc.at['split_%d' % (b+1), datasets[a]] = performance\n",
    "        performance_files_auroc.at['MEAN', datasets[a]] = np.nanmean(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())\n",
    "        performance_files_auroc.at['STERR', datasets[a]] = np.nanstd(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_auroc.to_csv('%s/_individual/auroc.csv' % output_folder)\n",
    "        \n",
    "        # shap tree explainer\n",
    "        explainer_independent = shap.TreeExplainer(bst, data=X_trainvalidation, feature_dependence='independent', model_output='probability')\n",
    "        explainer_dependent = shap.TreeExplainer(bst, data=X_trainvalidation, feature_dependence='tree_path_dependent', model_output='margin')\n",
    "        tree_limit = bst.best_ntree_limit\n",
    "\n",
    "        # load validation predictions\n",
    "        with open('_files/validation_xgb_%s.pickle' % timestamp,'rb') as f:\n",
    "            validation_predictions = pickle.load(f)\n",
    "        validation_y = np.concatenate(y_validation)\n",
    "        validation_X = pd.concat(X_validation)\n",
    "\n",
    "        # save results\n",
    "        with open('%s/_individual/%s/iter_%d.pickle' % (output_folder, datasets[a], b+1), 'wb') as f:\n",
    "            pickle.dump([validation_X, validation_y, validation_predictions, X_test, y_test, y_pred, explainer_independent, explainer_dependent, tree_limit], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
