{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'compareclassifiers_nohyperopt'\n",
    "datasets = [['clinical','gene_all','mutation_onehot_all']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate SHAP values and interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_shap_values = [False]\n",
    "compute_shap_interactions = [False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_trainvalidation_test = 20\n",
    "k_train_validation = 5\n",
    "early_stopping_size = 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgBoost stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hyperopt_iterations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1\n",
    "\n",
    "# implement seed\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_function(parameters):\n",
    "\n",
    "    # load data\n",
    "    with open('_files/data__.pickle', 'rb') as f:\n",
    "        X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # calculate performance\n",
    "    mean_validation_weightedlogloss = hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters)\n",
    "    \n",
    "    # return performance\n",
    "    return {'loss':mean_validation_weightedlogloss, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters):\n",
    "    \n",
    "    # initialize validation performance\n",
    "    validation_weightedlogloss = []\n",
    "    \n",
    "    # iterate over number of training/validation splits\n",
    "    for i in range(k_train_validation):\n",
    "\n",
    "        # xgb datasets\n",
    "        xgb_training = xgb.DMatrix(X_training_train[i], label=y_training_train[i])\n",
    "        xgb_earlystopping = xgb.DMatrix(X_earlystopping_train[i], label=y_earlystopping_train[i])\n",
    "        xgb_validation = xgb.DMatrix(X_validation[i], label=y_validation[i])\n",
    "\n",
    "        # parameters\n",
    "        param = {}\n",
    "        param['objective'] = 'multi:softprob'\n",
    "        param['num_class'] = len(datasets[a])\n",
    "        param['eval_metric'] = 'mlogloss'\n",
    "        param['seed'] = seed_\n",
    "        evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "        # train on training\n",
    "        bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "        # evaluate on validation\n",
    "        y_pred = bst.predict(xgb_validation, ntree_limit=bst.best_ntree_limit)\n",
    "        weightedlogloss = log_loss(y_validation[i], y_pred, labels=list(range(len(datasets[a]))))\n",
    "        validation_weightedlogloss.append(weightedlogloss)\n",
    "    \n",
    "    # average validation performance over all folds\n",
    "    mean_validation_weightedlogloss = np.mean(validation_weightedlogloss) + np.std(validation_weightedlogloss)/np.sqrt(len(validation_weightedlogloss))\n",
    "    return mean_validation_weightedlogloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_values(explainer, X_test, tree_limit):\n",
    "    \n",
    "    # compute values\n",
    "    shap_values = explainer.shap_values(X_test, tree_limit=tree_limit)\n",
    "    \n",
    "    # merge features\n",
    "    if len(categorical_conversion) > 0:\n",
    "        shap_values_ = np.zeros((X_test.shape[0], len(merged_features[c])))\n",
    "        for i, feature in enumerate(merged_features[c]):\n",
    "            if feature not in categorical_conversion:\n",
    "                shap_values_[:,i] = shap_values[:,features[c].index(feature)]\n",
    "            else:\n",
    "                find_indices = [j for j,x in enumerate(features[c]) if x.split(' | ')[0] == feature]\n",
    "                shap_values_[:,i] = shap_values[:,find_indices].sum(axis=1)\n",
    "        shap_values = shap_values_.copy()\n",
    "\n",
    "    # return results\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_interactions(explainer, X_test, tree_limit):\n",
    "    \n",
    "    # compute values\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X_test, tree_limit=tree_limit)[0]\n",
    "\n",
    "    # merge features\n",
    "    if len(categorical_conversion) > 0:\n",
    "        shap_interaction_values_ = np.zeros((len(merged_features[c]), len(features[c])))\n",
    "        for i,feature in enumerate(merged_features[c]):\n",
    "            if feature not in categorical_conversion:\n",
    "                shap_interaction_values_[i,:] = shap_interaction_values[features[c].index(feature),:]\n",
    "            else:\n",
    "                find_indices = [j for j,x in enumerate(features[c]) if x.split(' | ')[0] == feature]\n",
    "                shap_interaction_values_[i,:] = shap_interaction_values[find_indices,:].sum(axis=0)\n",
    "        shap_interaction_values = np.zeros((len(merged_features[c]), len(merged_features[c])))\n",
    "        for i,feature in enumerate(merged_features[c]):\n",
    "            if feature not in categorical_conversion:\n",
    "                shap_interaction_values[:,i] = shap_interaction_values_[:,features[c].index(feature)]\n",
    "            else:\n",
    "                find_indices = [j for j,x in enumerate(features[c]) if x.split(' | ')[0] == feature]\n",
    "                shap_interaction_values[:,i] = shap_interaction_values_[:,find_indices].sum(axis=1)\n",
    "\n",
    "    # return results\n",
    "    return shap_interaction_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_y(y):\n",
    "    \n",
    "    dummy_y_ = [[],[]]\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            dummy_y_[0].append(1)\n",
    "            dummy_y_[1].append(0)\n",
    "        else:\n",
    "            dummy_y_[0].append(0)\n",
    "            dummy_y_[1].append(1)\n",
    "    dummy_y_ = np.array(dummy_y_).T\n",
    "    return dummy_y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset names\n",
    "dataset_names = []\n",
    "for a in range(len(datasets)):\n",
    "    dataset_names.append('+'.join(datasets[a]))\n",
    "    os.mkdir('%s/%s' % (output_folder, dataset_names[a]))\n",
    "    \n",
    "# performance files\n",
    "performance_files_weightedlogloss = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_weightedlogloss.to_csv('%s/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "performance_files_balancedaccuracy = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_balancedaccuracy.to_csv('%s/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "performance_files_auroc = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_auroc.to_csv('%s/auroc.csv' % output_folder)\n",
    "\n",
    "performance_files_sensitivity_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_sensitivity_50.to_csv('%s/sensitivity_50.csv' % output_folder)\n",
    "\n",
    "performance_files_specificity_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_specificity_50.to_csv('%s/specificity_50.csv' % output_folder)\n",
    "\n",
    "performance_files_ppv_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_ppv_50.to_csv('%s/ppv_50.csv' % output_folder)\n",
    "\n",
    "performance_files_npv_50 = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_npv_50.to_csv('%s/npv_50.csv' % output_folder)\n",
    "\n",
    "performance_files_optimal_threshold = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_optimal_threshold.to_csv('%s/optimal_threshold.csv' % output_folder)\n",
    "\n",
    "performance_files_sensitivity_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_sensitivity_optimal.to_csv('%s/sensitivity_optimal.csv' % output_folder)\n",
    "\n",
    "performance_files_specificity_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_specificity_optimal.to_csv('%s/specificity_optimal.csv' % output_folder)\n",
    "\n",
    "performance_files_ppv_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_ppv_optimal.to_csv('%s/ppv_optimal.csv' % output_folder)\n",
    "\n",
    "performance_files_npv_optimal = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=dataset_names)\n",
    "performance_files_npv_optimal.to_csv('%s/npv_optimal.csv' % output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "DATASET: clinical+gene_all+mutation_onehot_all\n",
      "-------------------------\n",
      "Split 1\n",
      "clinical: 531/732 - 72.54% - 102 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 728/732 [00:23<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 125/732 - 17.08% - 113 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 725/732 [00:42<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 76/732 - 10.38% - 144 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.25s/it, best loss: 0.8035304097419645]\n",
      "Split 2\n",
      "clinical: 404/732 - 55.19% - 114 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 718/732 [00:26<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 260/732 - 35.52% - 194 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [00:45<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 68/732 - 9.29% - 157 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.53s/it, best loss: 0.9490950869450593]\n",
      "Split 3\n",
      "clinical: 408/732 - 55.74% - 102 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|=================== | 709/732 [00:28<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 245/732 - 33.47% - 218 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 720/732 [00:51<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 79/732 - 10.79% - 185 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.08s/it, best loss: 0.8835333376975378]\n",
      "Split 4\n",
      "clinical: 426/732 - 58.20% - 57 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 715/732 [00:25<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 225/732 - 30.74% - 152 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [00:44<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 81/732 - 11.07% - 167 features\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.97s/it, best loss: 0.934064617839539]\n",
      "Split 5\n",
      "clinical: 388/732 - 53.01% - 93 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|=================== | 710/732 [00:29<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 266/732 - 36.34% - 226 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 722/732 [00:50<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 78/732 - 10.66% - 202 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it, best loss: 0.9817525144870795]\n",
      "Split 6\n",
      "clinical: 416/732 - 56.83% - 44 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 725/732 [00:20<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 229/732 - 31.28% - 62 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [00:54<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 87/732 - 11.89% - 207 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.62s/it, best loss: 0.9193076449849766]\n",
      "Split 7\n",
      "clinical: 366/732 - 50.00% - 96 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|=================== | 710/732 [00:20<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 272/732 - 37.16% - 78 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 723/732 [00:38<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 94/732 - 12.84% - 157 features\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.64s/it, best loss: 0.952499090121614]\n",
      "Split 8\n",
      "clinical: 445/732 - 60.79% - 102 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 722/732 [00:27<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 199/732 - 27.19% - 192 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 720/732 [00:58<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 88/732 - 12.02% - 193 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.97s/it, best loss: 0.8992315690825557]\n",
      "Split 9\n",
      "clinical: 418/732 - 57.10% - 90 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 717/732 [00:25<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 227/732 - 31.01% - 144 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [00:42<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 87/732 - 11.89% - 161 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.86s/it, best loss: 0.9427611185602542]\n",
      "Split 10\n",
      "clinical: 418/732 - 57.10% - 90 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 723/732 [00:24<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 226/732 - 30.87% - 121 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [00:28<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 88/732 - 12.02% - 66 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it, best loss: 0.9356425442699213]\n",
      "Split 11\n",
      "clinical: 527/732 - 71.99% - 75 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|=================== | 700/732 [00:21<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 127/732 - 17.35% - 101 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [00:47<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 78/732 - 10.66% - 181 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.58s/it, best loss: 0.7684505244893272]\n",
      "Split 12\n",
      "clinical: 482/732 - 65.85% - 56 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|=================== | 707/732 [00:24<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 192/732 - 26.23% - 152 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [00:51<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 58/732 - 7.92% - 203 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.66s/it, best loss: 0.8421103823644976]\n",
      "Split 13\n",
      "clinical: 507/732 - 69.26% - 75 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|=================== | 712/732 [00:24<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 154/732 - 21.04% - 121 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [00:46<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 71/732 - 9.70% - 156 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.86s/it, best loss: 0.8462376425946561]\n",
      "Split 14\n",
      "clinical: 426/732 - 58.20% - 101 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 724/732 [00:27<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 212/732 - 28.96% - 177 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|=================== | 704/732 [00:26<00:01]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 94/732 - 12.84% - 61 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.94s/it, best loss: 0.9339344447072885]\n",
      "Split 15\n",
      "clinical: 459/732 - 62.70% - 82 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 726/732 [00:27<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 193/732 - 26.37% - 178 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 727/732 [00:56<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 80/732 - 10.93% - 194 features\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.47s/it, best loss: 0.869283418098456]\n",
      "Split 16\n",
      "clinical: 453/732 - 61.89% - 79 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 731/732 [00:21<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 168/732 - 22.95% - 75 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 716/732 [00:39<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 111/732 - 15.16% - 147 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.61s/it, best loss: 0.9236980299661167]\n",
      "Split 17\n",
      "clinical: 435/732 - 59.43% - 85 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|=================== | 712/732 [00:27<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 210/732 - 28.69% - 217 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 723/732 [00:51<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 87/732 - 11.89% - 169 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.09s/it, best loss: 0.9395864983414468]\n",
      "Split 18\n",
      "clinical: 392/732 - 53.55% - 71 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 720/732 [00:22<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 255/732 - 34.84% - 106 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 722/732 [00:42<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 85/732 - 11.61% - 145 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.14s/it, best loss: 0.9533723914124774]\n",
      "Split 19\n",
      "clinical: 416/732 - 56.83% - 70 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 714/732 [00:22<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 197/732 - 26.91% - 119 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 716/732 [00:32<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 119/732 - 16.26% - 102 features\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.67s/it, best loss: 0.998454784801841]\n",
      "Split 20\n",
      "clinical: 451/732 - 61.61% - 62 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 719/732 [00:23<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_all: 194/732 - 26.50% - 133 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 729/732 [00:30<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutation_onehot_all: 87/732 - 11.89% - 100 features\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.84s/it, best loss: 0.8377058419848645]\n"
     ]
    }
   ],
   "source": [
    "# iterate over datasets\n",
    "for a in range(len(datasets)):\n",
    "    print('-------------------------')\n",
    "    print('DATASET: %s' % dataset_names[a])\n",
    "    print('-------------------------')\n",
    "    \n",
    "    # iterate over number of training+validation/testing splits\n",
    "    for b in range(n_splits_trainvalidation_test):\n",
    "    \n",
    "        print('Split %d' % (b+1))\n",
    "        \n",
    "        # load categorical conversion from each dataset\n",
    "        features = []\n",
    "        merged_features = []\n",
    "        with open('_datasets/%s.pickle' % datasets[a][0], 'rb') as f:\n",
    "            X_matrix, y_vector, categorical_conversion_old = pickle.load(f, encoding='latin1')\n",
    "        features.append(['%s # %s' % (datasets[a][0], x) for x in X_matrix.columns.tolist()])\n",
    "        categorical_conversion = {}\n",
    "        for key in categorical_conversion_old:\n",
    "            categorical_conversion['%s # %s' % (datasets[a][0], key)] = categorical_conversion_old[key]\n",
    "        if len(categorical_conversion) > 0:\n",
    "            merged_features.append([])\n",
    "            for feature in features[0]:\n",
    "                if feature.split(' | ')[0] not in categorical_conversion:\n",
    "                    merged_features[-1].append(feature)\n",
    "                elif feature.split(' | ')[0] not in merged_features[-1]:\n",
    "                    merged_features[-1].append(feature.split(' | ')[0])\n",
    "        else:\n",
    "            merged_features.append(features[0].copy())\n",
    "        for c in range(1,len(datasets[a])):\n",
    "            with open('_datasets/%s.pickle' % datasets[a][c], 'rb') as f:\n",
    "                X_matrix_, y_vector_, categorical_conversion_old = pickle.load(f, encoding='latin1')\n",
    "            features.append(['%s # %s' % (datasets[a][c], x) for x in X_matrix_.columns.tolist()])\n",
    "            categorical_conversion_ = {}\n",
    "            for key in categorical_conversion_old:\n",
    "                categorical_conversion_['%s # %s' % (datasets[a][c], key)] = categorical_conversion_old[key]\n",
    "            categorical_conversion = {**categorical_conversion, **categorical_conversion_}\n",
    "            if len(categorical_conversion_) > 0:\n",
    "                merged_features.append([])\n",
    "                for feature in features[c]:\n",
    "                    if feature.split(' | ')[0] not in categorical_conversion_:\n",
    "                        merged_features[-1].append(feature)\n",
    "                    elif feature.split(' | ')[0] not in merged_features[-1]:\n",
    "                        merged_features[-1].append(feature.split(' | ')[0])\n",
    "            else:\n",
    "                merged_features.append(features[c].copy())\n",
    "        \n",
    "        # load results from individual datasets\n",
    "        validation_X = []\n",
    "        validation_predictions = []\n",
    "        X_test = []\n",
    "        y_pred = []\n",
    "        explainers_independent = []\n",
    "        explainers_dependent = []\n",
    "        tree_limit = []\n",
    "        for c in range(len(datasets[a])):\n",
    "            with open('%s/_individual/%s/iter_%d.pickle' % (output_folder, datasets[a][c], b+1), 'rb') as f:\n",
    "                validation_X_, validation_y, validation_predictions_, X_test_, y_test, y_pred_, explainer_independent, explainer_dependent, tree_limit_ = pickle.load(f)\n",
    "            validation_X.append(validation_X_)\n",
    "            validation_predictions.append(validation_predictions_)\n",
    "            X_test.append(X_test_)\n",
    "            y_pred.append(y_pred_)\n",
    "            explainers_independent.append(explainer_independent)\n",
    "            explainers_dependent.append(explainer_dependent)\n",
    "            tree_limit.append(tree_limit_)\n",
    "\n",
    "        # combine predictions\n",
    "        validation_predictions = np.concatenate([x.reshape(-1,1) for x in validation_predictions], axis=1)\n",
    "        test_predictions = np.concatenate([x.reshape(-1,1) for x in y_pred], axis=1)\n",
    "        \n",
    "        # if more than one dataset\n",
    "        if len(datasets[a]) > 1:\n",
    "        \n",
    "            # validation best classifier\n",
    "            validation_best_classifier = []\n",
    "            for i in range(len(validation_y)):\n",
    "                if validation_y[i] == 0:\n",
    "                    validation_best_classifier.append(np.argmin(validation_predictions[i,:]))\n",
    "                elif validation_y[i] == 1:\n",
    "                    validation_best_classifier.append(np.argmax(validation_predictions[i,:]))\n",
    "            validation_best_classifier = np.array(validation_best_classifier)\n",
    "\n",
    "            # subset features that are in any of the models\n",
    "            features_in_models = []\n",
    "            for c in range(len(explainers_independent)):\n",
    "                shapval = explainers_independent[c].shap_values(validation_X[c], tree_limit=tree_limit[c]).mean(axis=0)\n",
    "                features_in_models.extend([features[c][i] for i in range(len(features[c])) if shapval[i] != 0])\n",
    "                print('%s: %d/%d - %0.2f%% - %d features' % (datasets[a][c], len([x for x in validation_best_classifier if x==c]), len(validation_best_classifier), len([x for x in validation_best_classifier if x==c])/len(validation_best_classifier)*100, len([features[c][i] for i in range(len(features[c])) if shapval[i] != 0])))\n",
    "\n",
    "            # get combined dataset with features in models\n",
    "            validation_X_all = pd.concat(validation_X, axis=1)[features_in_models]\n",
    "            X_test_all = pd.concat(X_test, axis=1)[features_in_models]\n",
    "\n",
    "            # separate training_full from earlystopping_full\n",
    "            training_index = []\n",
    "            earlystopping_index = []\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "            for training_, earlystopping_ in sss.split(validation_X_all, validation_best_classifier):\n",
    "                training_index.append(list(training_))\n",
    "                earlystopping_index.append(list(earlystopping_))\n",
    "            training_full_X = validation_X_all.iloc[training_index[0],]\n",
    "            earlystopping_full_X = validation_X_all.iloc[earlystopping_index[0],]\n",
    "            training_full_y = validation_best_classifier[training_index[0]]\n",
    "            earlystopping_full_y = validation_best_classifier[earlystopping_index[0]]\n",
    "\n",
    "            # separate full\n",
    "            sep1_index = []\n",
    "            sep2_index = []\n",
    "            skf = StratifiedKFold(n_splits=k_train_validation, shuffle=True, random_state=seed_)\n",
    "            for sep1_, sep2_ in skf.split(validation_X_all, validation_best_classifier):\n",
    "                sep1_index.append(list(sep1_))\n",
    "                sep2_index.append(list(sep2_))\n",
    "            sep1_X = []\n",
    "            sep2_X = []\n",
    "            sep1_y = []\n",
    "            sep2_y = []\n",
    "            for c in range(k_train_validation):\n",
    "                sep1_X.append(validation_X_all.iloc[sep1_index[c],])\n",
    "                sep2_X.append(validation_X_all.iloc[sep2_index[c],])\n",
    "                sep1_y.append(validation_best_classifier[sep1_index[c]])\n",
    "                sep2_y.append(validation_best_classifier[sep2_index[c]])\n",
    "\n",
    "            # separate training_sep1 from earlystopping_sep1\n",
    "            training_sep1_X = []\n",
    "            earlystopping_sep1_X = []\n",
    "            training_sep1_y = []\n",
    "            earlystopping_sep1_y = []\n",
    "            for c in range(k_train_validation):\n",
    "                training_index = []\n",
    "                earlystopping_index = []\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "                for training_, earlystopping_ in sss.split(sep1_X[c], sep1_y[c]):\n",
    "                    training_index.append(list(training_))\n",
    "                    earlystopping_index.append(list(earlystopping_))\n",
    "                training_sep1_X.append(sep1_X[c].iloc[training_index[0],])\n",
    "                earlystopping_sep1_X.append(sep1_X[c].iloc[earlystopping_index[0],])\n",
    "                training_sep1_y.append(sep1_y[c][training_index[0]])\n",
    "                earlystopping_sep1_y.append(sep1_y[c][earlystopping_index[0]])\n",
    "\n",
    "            # xgb parameter values\n",
    "            parameters = {\n",
    "                'dummy': hp.uniform('dummy', 0, 1),\n",
    "                         }\n",
    "            \n",
    "            # save info for hyperopt\n",
    "            with open('_files/data__.pickle','wb') as f:\n",
    "                pickle.dump([training_sep1_X, training_sep1_y, earlystopping_sep1_X, earlystopping_sep1_y, sep2_X, sep2_y], f)\n",
    "\n",
    "            # hyperopt to find best parameters\n",
    "            trials = Trials()\n",
    "            best = fmin(hyperopt_function, parameters, algo=tpe.suggest, max_evals=n_hyperopt_iterations, trials=trials, rstate=np.random.RandomState(seed_), verbose=0, show_progressbar=True)\n",
    "\n",
    "            # create classifier using best parameters\n",
    "            xgb_training = xgb.DMatrix(training_full_X, label=training_full_y)\n",
    "            xgb_earlystopping = xgb.DMatrix(earlystopping_full_X, label=earlystopping_full_y)\n",
    "            xgb_test = xgb.DMatrix(X_test_all)\n",
    "\n",
    "            param = {'objective':'multi:softprob', 'num_class':len(datasets[a]), 'eval_metric':'mlogloss', 'seed':seed_}\n",
    "            evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "            # train stacker\n",
    "            bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "            # get weights on testing set\n",
    "            weights = bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit)\n",
    "            \n",
    "            # calculate stacker performance - log loss\n",
    "            test_best_classifier = []\n",
    "            for i in range(len(y_test)):\n",
    "                if y_test[i] == 0:\n",
    "                    test_best_classifier.append(np.argmin(test_predictions[i,:]))\n",
    "                elif y_test[i] == 1:\n",
    "                    test_best_classifier.append(np.argmax(test_predictions[i,:]))\n",
    "            test_best_classifier = np.array(test_best_classifier)\n",
    "            \n",
    "            # save stacker predictions\n",
    "            with open('%s/%s/stacker_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "                pickle.dump([X_test, X_test_all, y_test, y_pred, test_predictions, weights, test_best_classifier, bst], f)\n",
    "            \n",
    "            # get predictions on test set\n",
    "            y_pred = []\n",
    "            for i in range(len(y_test)):\n",
    "                y_pred.append(np.average(test_predictions[i,:], weights=weights[i,:]))\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "        # if only one dataset\n",
    "        else:\n",
    "            y_pred = y_pred[0]\n",
    "            weights = np.array([1 for x in y_test]).reshape(-1,1)\n",
    "        \n",
    "        # save predictions\n",
    "        with open('%s/%s/predictions_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "            pickle.dump([X_test[0].index.tolist(), y_test, y_pred], f)\n",
    "        \n",
    "        # calculate test performance - weighted log loss\n",
    "        pos_weight = len([x for x in y_test if x==0])/len([x for x in y_test if x==1])\n",
    "        sample_weights = [pos_weight if x==1 else 1 for x in y_test]\n",
    "        performance = log_loss(y_test, y_pred, sample_weight=sample_weights)\n",
    "        performance_files_weightedlogloss.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_weightedlogloss.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_weightedlogloss.at['STERR', dataset_names[a]] = np.nanstd(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_weightedlogloss.to_csv('%s/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - balanced accuracy\n",
    "        y_pred_ = [1 if x>=0.5 else 0 for x in y_pred]\n",
    "        performance = balanced_accuracy_score(y_test, y_pred_)\n",
    "        performance_files_balancedaccuracy.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_balancedaccuracy.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_balancedaccuracy.at['STERR', dataset_names[a]] = np.nanstd(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_balancedaccuracy.to_csv('%s/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - auroc\n",
    "        y_pred_ = np.concatenate((np.array([1-x for x in y_pred]).reshape(-1,1), y_pred.reshape(-1,1)), axis=1)\n",
    "        performance = roc_auc_score(dummy_y(y_test), y_pred_)\n",
    "        performance_files_auroc.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_auroc.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_auroc.at['STERR', dataset_names[a]] = np.nanstd(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_auroc.to_csv('%s/auroc.csv' % output_folder)\n",
    "        \n",
    "        # tp, tn, fp, fn\n",
    "        tp = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]>0.5])\n",
    "        fp = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]>0.5])\n",
    "        tn = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]<0.5])\n",
    "        fn = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]<0.5])\n",
    "        \n",
    "        # calculate test performance - sensitivity - 50\n",
    "        performance = tp/(tp+fn)\n",
    "        performance_files_sensitivity_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_sensitivity_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_sensitivity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_sensitivity_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_sensitivity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_sensitivity_50.to_csv('%s/sensitivity_50.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - specificity - 50\n",
    "        performance = tn/(tn+fp)\n",
    "        performance_files_specificity_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_specificity_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_specificity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_specificity_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_specificity_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_specificity_50.to_csv('%s/specificity_50.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - ppv - 50\n",
    "        performance = tp/(tp+fp)\n",
    "        performance_files_ppv_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_ppv_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_ppv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_ppv_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_ppv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_ppv_50.to_csv('%s/ppv_50.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - npv - 50\n",
    "        performance = tn/(tn+fn)\n",
    "        performance_files_npv_50.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_npv_50.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_npv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_npv_50.at['STERR', dataset_names[a]] = np.nanstd(performance_files_npv_50.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_npv_50.to_csv('%s/npv_50.csv' % output_folder)\n",
    "        \n",
    "        # optimal threshold\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        youden = [(1-fpr[i])+tpr[i] for i in range(len(thresholds))]\n",
    "        top_index = []\n",
    "        top_threshold = []\n",
    "        for i in range(len(youden)):\n",
    "            if youden[i] == np.max(youden):\n",
    "                top_index.append(i)\n",
    "                top_threshold.append(thresholds[i])\n",
    "        distance_from_50 = [np.abs(x-0.5) for x in top_threshold]\n",
    "        top_index = top_index[np.argmin(distance_from_50)]\n",
    "        optimal_threshold = top_threshold[np.argmin(distance_from_50)]\n",
    "        performance_files_optimal_threshold.at['split_%d' % (b+1), dataset_names[a]] = optimal_threshold\n",
    "        performance_files_optimal_threshold.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_optimal_threshold.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_optimal_threshold.at['STERR', dataset_names[a]] = np.nanstd(performance_files_optimal_threshold.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_optimal_threshold.to_csv('%s/optimal_threshold.csv' % output_folder)\n",
    "        \n",
    "        # tp, tn, fp, fn\n",
    "        tp = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]>optimal_threshold])\n",
    "        fp = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]>optimal_threshold])\n",
    "        tn = len([i for i in range(len(y_test)) if y_test[i]==0 and y_pred[i]<optimal_threshold])\n",
    "        fn = len([i for i in range(len(y_test)) if y_test[i]==1 and y_pred[i]<optimal_threshold])\n",
    "        \n",
    "        # calculate test performance - sensitivity - optimal\n",
    "        performance = tp/(tp+fn)\n",
    "        performance_files_sensitivity_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_sensitivity_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_sensitivity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_sensitivity_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_sensitivity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_sensitivity_optimal.to_csv('%s/sensitivity_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - specificity - optimal\n",
    "        performance = tn/(tn+fp)\n",
    "        performance_files_specificity_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_specificity_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_specificity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_specificity_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_specificity_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_specificity_optimal.to_csv('%s/specificity_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - ppv - optimal\n",
    "        performance = tp/(tp+fp)\n",
    "        performance_files_ppv_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_ppv_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_ppv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_ppv_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_ppv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_ppv_optimal.to_csv('%s/ppv_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate test performance - npv - optimal\n",
    "        performance = tn/(tn+fn)\n",
    "        performance_files_npv_optimal.at['split_%d' % (b+1), dataset_names[a]] = performance\n",
    "        performance_files_npv_optimal.at['MEAN', dataset_names[a]] = np.nanmean(performance_files_npv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())\n",
    "        performance_files_npv_optimal.at['STERR', dataset_names[a]] = np.nanstd(performance_files_npv_optimal.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], dataset_names[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_npv_optimal.to_csv('%s/npv_optimal.csv' % output_folder)\n",
    "        \n",
    "        # calculate shap values\n",
    "        if compute_shap_values[a]:\n",
    "        \n",
    "            # initialize shap values\n",
    "            shap_values = []\n",
    "\n",
    "            # iterate over datasets\n",
    "            for c in range(len(datasets[a])):\n",
    "\n",
    "                # compute shap values\n",
    "                shapval = calculate_shap_values(explainers_independent[c], X_test[c], tree_limit[c])\n",
    "\n",
    "                # weight classifier shap values for each sample\n",
    "                for i in range(len(y_test)):\n",
    "                    shapval[i,:] *= weights[i,c]\n",
    "                shap_values.append(pd.DataFrame(data=shapval, index=X_test[c].index.tolist(), columns=merged_features[c]))\n",
    "\n",
    "            # merge multi-model shap values\n",
    "            if len(shap_values) > 1:\n",
    "                shap_values = pd.concat(shap_values, axis=1, sort=False)\n",
    "            else:\n",
    "                shap_values = shap_values[0]\n",
    "\n",
    "            # expected values\n",
    "            expected = []\n",
    "            for c in range(len(datasets[a])):\n",
    "                expected.append(explainers_independent[c].expected_value)\n",
    "            expected = np.array(expected)\n",
    "\n",
    "            # individual expected values\n",
    "            shap_expected = []\n",
    "            for i in range(len(y_test)):\n",
    "                shap_expected.append(np.average(expected, weights=weights[i,:]))\n",
    "\n",
    "            # save results\n",
    "            with open('%s/%s/shap_values_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "                pickle.dump([shap_expected, shap_values], f)\n",
    "                \n",
    "        # calculate shap interactions\n",
    "        if compute_shap_interactions[a]:\n",
    "                                                                                                      \n",
    "            # iterate over samples\n",
    "            for i in tqdm(range(len(y_test))):\n",
    "                                                                                                                                         \n",
    "                # initialize sample results\n",
    "                sample_shap = []\n",
    "                \n",
    "                # iterate over datasets\n",
    "                for c in range(len(datasets[a])):\n",
    "                        \n",
    "                    # calculate shap interaction values\n",
    "                    sample_shap.append(pd.DataFrame(data=weights[i,c] * calculate_shap_interactions(explainers_dependent[c], X_test[c].iloc[i:i+1,:], tree_limit[c]), index=merged_features[c], columns=merged_features[c]))\n",
    "                        \n",
    "                # merge multi-class shap values\n",
    "                if len(sample_shap) > 1:\n",
    "                    sample_shap_ = pd.concat(sample_shap, axis=1, sort=False).fillna(0)\n",
    "                else:\n",
    "                    sample_shap_ = sample_shap[0]\n",
    "                \n",
    "                # add to overall array\n",
    "                if i==0:\n",
    "                    shap_interaction_values = sample_shap_.copy()\n",
    "                else:\n",
    "                    shap_interaction_values += sample_shap_\n",
    "                \n",
    "            # divide by total number of samples\n",
    "            shap_interaction_values /= len(y_test)\n",
    "            \n",
    "            # save results\n",
    "            with open('%s/%s/shap_interactions_%d.pickle' % (output_folder,dataset_names[a],b+1) ,'wb') as f:\n",
    "                pickle.dump(shap_interaction_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
