{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'compareclassifiers_combined'\n",
    "\n",
    "if os.path.isdir(output_folder):\n",
    "    raise Exception('Already run!')\n",
    "else:\n",
    "    os.mkdir(output_folder)\n",
    "    os.mkdir('%s/_individual' % output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['combined_cgm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hyperopt_iterations = 2**8\n",
    "optimize_learning_rate = True\n",
    "default_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_trainvalidation_test = 20\n",
    "test_size = 0.2\n",
    "k_train_validation = 5\n",
    "early_stopping_size = 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1\n",
    "\n",
    "# implement seed\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "\n",
    "# timestamp\n",
    "timestamp = 1912111158"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_function(parameters):\n",
    "\n",
    "    # load data\n",
    "    with open('_files/data_%s.pickle' % timestamp, 'rb') as f:\n",
    "        X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # calculate performance\n",
    "    mean_validation_weightedlogloss, validation_pred = hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters)\n",
    "    gc.collect()\n",
    "    \n",
    "    # save validation predictions if best classifier\n",
    "    with open('_files/validation_%s.pickle' % timestamp,'rb') as f:\n",
    "        best_weightedlogloss = pickle.load(f)\n",
    "    if mean_validation_weightedlogloss < best_weightedlogloss:\n",
    "        with open('_files/validation_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump(mean_validation_weightedlogloss, f)\n",
    "        with open('_files/validation_xgb_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump(validation_pred, f)\n",
    "    \n",
    "    # return performance\n",
    "    return {'loss':mean_validation_weightedlogloss, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_performance(X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation, parameters):\n",
    "    \n",
    "    # initialize validation performance and predictions\n",
    "    validation_weightedlogloss = []\n",
    "    validation_pred = []\n",
    "    \n",
    "    # iterate over number of training/validation splits\n",
    "    for i in range(k_train_validation):\n",
    "        \n",
    "        # positive weight\n",
    "        pos_weight = len([x for x in y_training_train[i] if x==0])/len([x for x in y_training_train[i] if x==1])\n",
    "\n",
    "        # xgb datasets\n",
    "        xgb_training = xgb.DMatrix(X_training_train[i], label=y_training_train[i])\n",
    "        xgb_earlystopping = xgb.DMatrix(X_earlystopping_train[i], label=y_earlystopping_train[i])\n",
    "        xgb_validation = xgb.DMatrix(X_validation[i], label=y_validation[i])\n",
    "\n",
    "        # parameters\n",
    "        param = parameters.copy()\n",
    "        param['objective'] = 'binary:logistic'\n",
    "        param['eval_metric'] = 'logloss'\n",
    "        param['scale_pos_weight'] = pos_weight\n",
    "        param['seed'] = seed_\n",
    "        evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "        # train on training\n",
    "        bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "        # evaluate on validation\n",
    "        y_pred = bst.predict(xgb_validation, ntree_limit=bst.best_ntree_limit)\n",
    "        pos_weight = len([x for x in y_validation[i] if x==0])/len([x for x in y_validation[i] if x==1])\n",
    "        sample_weights = [pos_weight if x==1 else 1 for x in y_validation[i]]\n",
    "        weightedlogloss = log_loss(y_validation[i], y_pred, sample_weight=sample_weights)\n",
    "        validation_weightedlogloss.append(weightedlogloss)\n",
    "        validation_pred.append(y_pred)\n",
    "    \n",
    "    # average validation performance over all folds\n",
    "    mean_validation_weightedlogloss = np.mean(validation_weightedlogloss) + np.std(validation_weightedlogloss)/np.sqrt(len(validation_weightedlogloss))\n",
    "    return mean_validation_weightedlogloss, np.concatenate(validation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_y(y):\n",
    "    \n",
    "    dummy_y_ = [[],[]]\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            dummy_y_[0].append(1)\n",
    "            dummy_y_[1].append(0)\n",
    "        else:\n",
    "            dummy_y_[0].append(0)\n",
    "            dummy_y_[1].append(1)\n",
    "    dummy_y_ = np.array(dummy_y_).T\n",
    "    return dummy_y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over datasets\n",
    "for a in range(len(datasets)):\n",
    "    \n",
    "    # folder\n",
    "    os.mkdir('%s/_individual/%s' % (output_folder, datasets[a]))\n",
    "    \n",
    "# performance files\n",
    "performance_files_weightedlogloss = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=datasets)\n",
    "performance_files_weightedlogloss.to_csv('%s/_individual/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "performance_files_balancedaccuracy = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=datasets)\n",
    "performance_files_balancedaccuracy.to_csv('%s/_individual/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "performance_files_auroc = pd.DataFrame(index=['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)]+['MEAN','STERR'], columns=datasets)\n",
    "performance_files_auroc.to_csv('%s/_individual/auroc.csv' % output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "DATASET: combined_cgm\n",
      "-------------------------\n",
      "Split 1\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:03:23<00:00, 154.76s/it, best loss: 0.533209550838605]\n",
      "Split 2\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:44:50<00:00, 108.36s/it, best loss: 0.545901844979301]\n",
      "Split 3\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [13:54:32<00:00, 237.70s/it, best loss: 0.5526418351590279]\n",
      "Split 4\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [14:41:22<00:00, 163.69s/it, best loss: 0.5543833939280297]\n",
      "Split 5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [16:02:17<00:00, 193.03s/it, best loss: 0.5580812663439458]\n",
      "Split 6\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [11:25:39<00:00, 166.43s/it, best loss: 0.5494214220265365]\n",
      "Split 7\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:55:28<00:00, 179.04s/it, best loss: 0.5457852872727795]\n",
      "Split 8\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [12:32:43<00:00, 173.71s/it, best loss: 0.5682909088990648]\n",
      "Split 9\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [9:44:58<00:00, 133.88s/it, best loss: 0.5491458321297352]\n",
      "Split 10\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [5:19:25<00:00, 65.83s/it, best loss: 0.5207156346989865]\n",
      "Split 11\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [6:46:41<00:00, 106.25s/it, best loss: 0.5445777187987973]\n",
      "Split 12\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [7:28:40<00:00, 73.33s/it, best loss: 0.5565760057045541]\n",
      "Split 13\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [8:09:08<00:00, 118.55s/it, best loss: 0.5637169259674552]\n",
      "Split 14\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [9:17:15<00:00, 93.59s/it, best loss: 0.5528093436959892]\n",
      "Split 15\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [10:20:56<00:00, 138.81s/it, best loss: 0.5405486168623334]\n",
      "Split 16\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [8:15:33<00:00, 146.94s/it, best loss: 0.5583470425430189]\n",
      "Split 17\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [6:29:05<00:00, 88.74s/it, best loss: 0.5475533937706271]\n",
      "Split 18\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [5:47:27<00:00, 91.00s/it, best loss: 0.5449649802328499]\n",
      "Split 19\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [7:24:16<00:00, 94.70s/it, best loss: 0.5424286579592956]\n",
      "Split 20\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [9:29:10<00:00, 156.23s/it, best loss: 0.5565036119395804]\n"
     ]
    }
   ],
   "source": [
    "# iterate over datasets\n",
    "for a in range(len(datasets)):\n",
    "    print('-------------------------')\n",
    "    print('DATASET: %s' % datasets[a])\n",
    "    print('-------------------------')\n",
    "    \n",
    "    # load dataset\n",
    "    with open('_datasets/%s.pickle' % datasets[a], 'rb') as f:\n",
    "        X_matrix, y_vector, categorical_conversion = pickle.load(f, encoding='latin1')\n",
    "    X_matrix.columns = ['%s # %s' % (datasets[a], feature) for feature in X_matrix.columns.tolist()]\n",
    "\n",
    "    # divide train+validation from testing\n",
    "    trainvalidation_index = []\n",
    "    test_index = []\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits_trainvalidation_test, test_size=test_size, random_state=seed_)\n",
    "    for trainvalidation_, test_ in sss.split(X_matrix, y_vector):\n",
    "        trainvalidation_index.append(list(trainvalidation_))\n",
    "        test_index.append(list(test_))\n",
    "\n",
    "    # iterate over number of training+validation/testing splits\n",
    "    for b in range(n_splits_trainvalidation_test):\n",
    "        print('Split %d' % (b+1))\n",
    "        \n",
    "        # separate train+validation and testing\n",
    "        X_trainvalidation = X_matrix.iloc[trainvalidation_index[b],]\n",
    "        X_test = X_matrix.iloc[test_index[b],]\n",
    "        y_trainvalidation = y_vector[trainvalidation_index[b]]\n",
    "        y_test = y_vector[test_index[b]]\n",
    "\n",
    "        # separate training_trainvalidation from earlystopping_trainvalidation\n",
    "        training_index = []\n",
    "        earlystopping_index = []\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "        for training_, earlystopping_ in sss.split(X_trainvalidation, y_trainvalidation):\n",
    "            training_index.append(list(training_))\n",
    "            earlystopping_index.append(list(earlystopping_))\n",
    "        X_training_trainvalidation = X_trainvalidation.iloc[training_index[0],]\n",
    "        X_earlystopping_trainvalidation = X_trainvalidation.iloc[earlystopping_index[0],]\n",
    "        y_training_trainvalidation = y_trainvalidation[training_index[0]]\n",
    "        y_earlystopping_trainvalidation = y_trainvalidation[earlystopping_index[0]]\n",
    "        \n",
    "        # divide train from validation\n",
    "        train_index = []\n",
    "        validation_index = []\n",
    "        skf = StratifiedKFold(n_splits=k_train_validation, shuffle=True, random_state=seed_)\n",
    "        for train_, validation_ in skf.split(X_trainvalidation, y_trainvalidation):\n",
    "            train_index.append(list(train_))\n",
    "            validation_index.append(list(validation_))\n",
    "\n",
    "        # separate train and validation\n",
    "        X_train = []\n",
    "        X_validation = []\n",
    "        y_train = []\n",
    "        y_validation = []\n",
    "        for c in range(k_train_validation):\n",
    "            X_train.append(X_trainvalidation.iloc[train_index[c],])\n",
    "            X_validation.append(X_trainvalidation.iloc[validation_index[c],])\n",
    "            y_train.append(y_trainvalidation[train_index[c]])\n",
    "            y_validation.append(y_trainvalidation[validation_index[c]])\n",
    "        \n",
    "        # separate training_train from earlystopping_train\n",
    "        X_training_train = []\n",
    "        X_earlystopping_train = []\n",
    "        y_training_train = []\n",
    "        y_earlystopping_train = []\n",
    "        for c in range(k_train_validation):\n",
    "            training_index = []\n",
    "            earlystopping_index = []\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=early_stopping_size, random_state=seed_)\n",
    "            for training_, earlystopping_ in sss.split(X_train[c], y_train[c]):\n",
    "                training_index.append(list(training_))\n",
    "                earlystopping_index.append(list(earlystopping_))\n",
    "            X_training_train.append(X_train[c].iloc[training_index[0],])\n",
    "            X_earlystopping_train.append(X_train[c].iloc[earlystopping_index[0],])\n",
    "            y_training_train.append(y_train[c][training_index[0]])\n",
    "            y_earlystopping_train.append(y_train[c][earlystopping_index[0]])\n",
    "            \n",
    "        # initialize test predictions\n",
    "        classifier_test_predictions = []\n",
    "        \n",
    "        # initialize shap explainers\n",
    "        explainers = []\n",
    "        \n",
    "        # xgb parameters\n",
    "        parameters = {\n",
    "            'gamma': hp.loguniform('gamma', np.log(0.0001), np.log(5)) - 0.0001,\n",
    "            'max_depth': scope.int(hp.uniform('max_depth', 1, 11)),\n",
    "            'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "            'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n",
    "            'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(4)),\n",
    "            'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)) - 0.0001\n",
    "                     }\n",
    "        if optimize_learning_rate:\n",
    "            parameters['eta'] = hp.loguniform('eta', np.log(0.01), np.log(0.5))\n",
    "        else:\n",
    "            parameters['eta'] = hp.choice('eta', [default_learning_rate])\n",
    "\n",
    "        # save info for hyperopt\n",
    "        with open('_files/validation_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump(1000., f)\n",
    "        with open('_files/data_%s.pickle' % timestamp,'wb') as f:\n",
    "            pickle.dump([X_training_train, y_training_train, X_earlystopping_train, y_earlystopping_train, X_validation, y_validation], f)\n",
    "\n",
    "        # hyperopt to find best parameters\n",
    "        trials = Trials()\n",
    "        best = fmin(hyperopt_function, parameters, algo=tpe.suggest, max_evals=n_hyperopt_iterations, trials=trials, rstate=np.random.RandomState(seed_), verbose=0, show_progressbar=True)\n",
    "            \n",
    "        # create classifier using best parameters\n",
    "        pos_weight = len([x for x in y_training_trainvalidation if x==0])/len([x for x in y_training_trainvalidation if x==1])\n",
    "            \n",
    "        # xgb datasets\n",
    "        xgb_training = xgb.DMatrix(X_training_trainvalidation, label=y_training_trainvalidation)\n",
    "        xgb_earlystopping = xgb.DMatrix(X_earlystopping_trainvalidation, label=y_earlystopping_trainvalidation)\n",
    "        xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # parameters\n",
    "        if optimize_learning_rate:\n",
    "            param = {'objective':'binary:logistic', 'eval_metric':'logloss', 'scale_pos_weight':pos_weight, 'seed':seed_, 'eta':best['eta'], 'gamma':best['gamma'], 'max_depth':int(best['max_depth']), 'subsample':best['subsample'], 'colsample_bytree':best['colsample_bytree'], 'colsample_bylevel':best['colsample_bylevel'], 'reg_lambda':best['reg_lambda'], 'reg_alpha':best['reg_alpha']}\n",
    "        else:\n",
    "            param = {'objective':'binary:logistic', 'eval_metric':'logloss', 'scale_pos_weight':pos_weight, 'seed':seed_, 'eta':default_learning_rate, 'gamma':best['gamma'], 'max_depth':int(best['max_depth']), 'subsample':best['subsample'], 'colsample_bytree':best['colsample_bytree'], 'colsample_bylevel':best['colsample_bylevel'], 'reg_lambda':best['reg_lambda'], 'reg_alpha':best['reg_alpha']}\n",
    "        evallist = [(xgb_training, 'train'), (xgb_earlystopping, 'eval')]\n",
    "\n",
    "        # train on training+validation\n",
    "        bst = xgb.train(param, xgb_training, num_boost_round=10000, evals=evallist, early_stopping_rounds=10, verbose_eval=False)\n",
    "                \n",
    "        # predicted probabilities on test set\n",
    "        y_pred = bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "        # calculate test performance - weighted log loss\n",
    "        pos_weight = len([x for x in y_test if x==0])/len([x for x in y_test if x==1])\n",
    "        sample_weights = [pos_weight if x==1 else 1 for x in y_test]\n",
    "        performance = log_loss(y_test, y_pred, sample_weight=sample_weights)\n",
    "        performance_files_weightedlogloss.at['split_%d' % (b+1), datasets[a]] = performance\n",
    "        performance_files_weightedlogloss.at['MEAN', datasets[a]] = np.nanmean(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())\n",
    "        performance_files_weightedlogloss.at['STERR', datasets[a]] = np.nanstd(performance_files_weightedlogloss.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_weightedlogloss.to_csv('%s/_individual/weightedlogloss.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - balanced accuracy\n",
    "        y_pred_ = [1 if x>=0.5 else 0 for x in y_pred]\n",
    "        performance = balanced_accuracy_score(y_test, y_pred_)\n",
    "        performance_files_balancedaccuracy.at['split_%d' % (b+1), datasets[a]] = performance\n",
    "        performance_files_balancedaccuracy.at['MEAN', datasets[a]] = np.nanmean(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())\n",
    "        performance_files_balancedaccuracy.at['STERR', datasets[a]] = np.nanstd(performance_files_balancedaccuracy.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_balancedaccuracy.to_csv('%s/_individual/balancedaccuracy.csv' % output_folder)\n",
    "\n",
    "        # calculate test performance - auroc\n",
    "        y_pred_ = np.concatenate((np.array([1-x for x in y_pred]).reshape(-1,1), y_pred.reshape(-1,1)), axis=1)\n",
    "        performance = roc_auc_score(dummy_y(y_test), y_pred_)\n",
    "        performance_files_auroc.at['split_%d' % (b+1), datasets[a]] = performance\n",
    "        performance_files_auroc.at['MEAN', datasets[a]] = np.nanmean(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())\n",
    "        performance_files_auroc.at['STERR', datasets[a]] = np.nanstd(performance_files_auroc.loc[['split_%d' % x for x in range(1,n_splits_trainvalidation_test+1)], datasets[a]].values.tolist())/np.sqrt(b+1)\n",
    "        performance_files_auroc.to_csv('%s/_individual/auroc.csv' % output_folder)\n",
    "        \n",
    "        # shap tree explainer\n",
    "        explainer_independent = shap.TreeExplainer(bst, data=X_trainvalidation, feature_dependence='independent', model_output='probability')\n",
    "        explainer_dependent = shap.TreeExplainer(bst, data=X_trainvalidation, feature_dependence='tree_path_dependent', model_output='margin')\n",
    "        tree_limit = bst.best_ntree_limit\n",
    "\n",
    "        # load validation predictions\n",
    "        with open('_files/validation_xgb_%s.pickle' % timestamp,'rb') as f:\n",
    "            validation_predictions = pickle.load(f)\n",
    "        validation_y = np.concatenate(y_validation)\n",
    "        validation_X = pd.concat(X_validation)\n",
    "\n",
    "        # save results\n",
    "        with open('%s/_individual/%s/iter_%d.pickle' % (output_folder, datasets[a], b+1), 'wb') as f:\n",
    "            pickle.dump([validation_X, validation_y, validation_predictions, X_test, y_test, y_pred, explainer_independent, explainer_dependent, tree_limit], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
